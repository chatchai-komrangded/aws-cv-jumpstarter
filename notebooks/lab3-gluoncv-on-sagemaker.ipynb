{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using GluonCV on Amazon SageMaker\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation and Exploration](#[Optional]-Data-Preparation-and-Exploration)\n",
    "4. [Experimentation and Prototyping](#Experimentation-and-Prototyping)\n",
    "5. [Train the model](#Train-the-model)\n",
    "6. [Host the model](#Host-the-model)\n",
    "7. [Test the model](#Test-the-model)\n",
    "8. [Clean up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Object detection is the process of identifying and localizing objects in an image. A typical object detection solution takes an image as input and provides a bounding box on the image where an object of interest is found.  It also identifies what type of object the box encapsulates.  To create such a solution, we need to acquire and process a traning dataset, create and setup a training job for the alorithm so that it can learn about the dataset. Finally, we can then host the trained model in an endpoint, to which we can supply images.\n",
    "\n",
    "This notebook is an end-to-end example showing how GluonCV's Object Detection algorithms can be used with Amazon SageMaker, and a public dataset of bird images. We demonstrate how to train and to host an object detection model based on the [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset.  GluonCV provides variants of SSD, YOLOv3 and Faster RCNN algorithms for object detection algorithm. In this notebook we demonstrate YOLOv3 on mobilenet1.0 with weights pretrained on the [VOC](http://host.robots.ox.ac.uk/pascal/VOC/) dataset.\n",
    "\n",
    "![Sample results detecting a pair of goldfinch on a feeder](./goldfinch_detections.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before preparing the data, there are some initial steps required for setup.\n",
    "\n",
    "First, you should be running this notebook on a **ml.c5d.4xlarge** or larger instance if you plan to run the training script locally on your notebook. Otherwise, you'll need to lower the batch size to avoid out-of-memory errors.\n",
    "\n",
    "Any GPU instances like the ml.p3.2xlarge will do, and in practice, will be the ideal choice for a notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to import the depedencies on this notebook. Most of the libraries that you need for MXNet and Gluon development are already packaged into the conda_mxnet_p36 environment.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to install some dependencies that might not be included in your MXNet environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet, MXNetModel\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "SOURCE_DIR = '../scripts/yolov3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GluonCV isn't pre-installed into this environment. However, most of the dependencies required by GluonCV have already been packaged into the **conda_mxnet_p36** environment. \n",
    "\n",
    "This notebook has requirements that we will have to install manually:\n",
    "* MXNet 1.6+\n",
    "* GluonCV libraries\n",
    "\n",
    "Installation instructions for GluonCV are provided [here](https://gluon-cv.mxnet.io/#installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MXNet dependencies are GPU dependent, so we'll check for the presence of GPU devnices first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "try:\n",
    "    a = mx.nd.zeros((1,), ctx=mx.gpu(0))\n",
    "    ctx = [mx.gpu(0)]\n",
    "    print('GPU device is available')\n",
    "except:\n",
    "    ctx = [mx.cpu()]\n",
    "    USE_GPU = False\n",
    "    print('Using CPU on local machine. GPU device was not detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we...\n",
    "* Upgrade MXNet 1.6 for CUDA 10.1 with the appropriate libraries depending on whether GPU is available.\n",
    "* Install GluonCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMEWORK_VERSION = \"1.6.0\"\n",
    "GLUONCV_COMPATIBILITY = {\n",
    "    \"1.5.0\":\"0.6.0\",\n",
    "    \"1.5.1\":\"0.6.0\",\n",
    "    \"1.6.0\":\"0.8.0\"\n",
    "}\n",
    "start = time.time()\n",
    "\n",
    "if mx.__version__ < FRAMEWORK_VERSION : \n",
    "    if USE_GPU :\n",
    "        cmd = 'pip install mxnet-cu101mkl=={} --upgrade'.format(FRAMEWORK_VERSION)\n",
    "    else :\n",
    "        cmd ='pip install mxnet-mkl=={} --upgrade'.format(FRAMEWORK_VERSION)\n",
    "\n",
    "    print(\"Changing MXNet: {}\".format(cmd))\n",
    "    os.system(cmd)\n",
    "\n",
    "cmd = \"pip install gluoncv=={} --upgrade\".format(GLUONCV_COMPATIBILITY[FRAMEWORK_VERSION])\n",
    "print(\"Install GluonCV: {}\".format(cmd))\n",
    "os.system(cmd)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"GluonCV dependencies were installed in {:3f} seconds.\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gluoncv should now be available for us. Import our required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluoncv as gcv\n",
    "from gluoncv.utils import download, viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next session, we'll create an optimized dataset (RecordIO format) for training our YOLOv3 model. The process can take 15 minutes. \n",
    "\n",
    "It's up to you whether you like to go through the steps of pre-processing your dataset into RecordIO format. Alternatively, I've provided pre-processed datasets for you to use.\n",
    "\n",
    "Run the cell below to set the locations of the pre-processed train and validation datasets. Note that MXNet doesn't require RecordIO format, but there are various benefits as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TRAINING_DATASET = 's3://reinvent2018-sagemaker-pytorch/datasets/caltech-birds/recordio/train'\n",
    "SAMPLE_TEST_DATASET = 's3://reinvent2018-sagemaker-pytorch/datasets/caltech-birds/recordio/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] Data Preparation and Exploration\n",
    "\n",
    "---\n",
    "This section is **optional**. For the purpose of making this lab as efficient as possible, data sets have already been prepared for you in MXNet [RecordIO format](https://mxnet.incubator.apache.org/versions/master/faq/recordio.html), which has various [benefits](https://mxnet.incubator.apache.org/versions/master/faq/recordio.html) including performance enhancements. The following are steps that were taken to produce training and validation samples in RecordIO format. Take note of the utility functions that MXNet provides for format conversion as well as the native data loaders. These are great features that reduce data wrangling work, and aren't provided by most frameworks.\n",
    "\n",
    "---\n",
    "This [link](https://cv-workshop-2.notebook.us-west-2.sagemaker.aws/notebooks/aws-cv-jumpstarter/notebooks/lab3b-optional-generate-recordio-files.ipynb) will launch another notebook, which will guide you through this optional section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation and Prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook environment is an ideal environment for rapid prototyping and experimentation. Run the cell below to preview the YOLOv3 training script that we will use to train the model.\n",
    "\n",
    "The script is no different than one you would run outside of the Amazon SageMaker environment. The only exception is that are conditions in the script to set parameters to environment variables that will be present when we run remote SageMaker training. They're only necessary if you plan to refactor your script to take advantage of the zero-setup SageMaker training capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize $SOURCE_DIR/gluoncv_yolo3-train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below runs the above script on this notebook. The script is only configure to run for one epoch because it can take over 5 minutes to run a single epoch on CPU.\n",
    "\n",
    "The YOLOv3 script has been configured to use mobilenet1.0 (versus darknet), and will be initialized with weights pretrained on the VOC dataset.\n",
    "\n",
    "It's **optional** to run this script locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir ./models\n",
    "!python $SOURCE_DIR/gluoncv_yolo3-train.py \\\n",
    "--model-dir ./models \\\n",
    "--train ../data/caltech_birds_sample/recordio/train \\\n",
    "--test ../data/caltech_birds_sample/recordio/test \\\n",
    "--lr 0.001 \\\n",
    "--opt adam \\\n",
    "--local True \\\n",
    "--pre-trained-weights VOC \\\n",
    "--num-workers 1 \\\n",
    "--save-format symbolic \\\n",
    "--network mobilenet1.0 --syncbn True --batch-size 4 --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate your Scripts\n",
    "\n",
    "You may eventually need to perform large-scale training that is beyond what want can be done from your local notebook. Before you leverage SageMaker traiing capabilities, you can run your training script in **\"local mode\"** to validate that your script will run within the SageMaker training environment.\n",
    "\n",
    "Local mode will deploy a SageMaker training container locally, and run your script. In the following steps we will setup our notebook to train in \"local mode.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies on SageMaker Managed Containers\n",
    "\n",
    "---\n",
    "\n",
    "One of the main benefits of leveraging the \"Bring-Your-Own-Script\" (BYOS) integration strategy is that it reduces alot of heavy-lifting compared to the alternative. For instance, if you want to integrate a custom algorithm in R with Amazon SageMaker, it is feasible. However, at this point in time, the BYOS integration option doesn't supported in R. Integrating your R algorithm would require you to build  an Amazon SageMaker compatible container to house your custom [algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html).\n",
    "\n",
    "MXNet, however, supports BYOS--Amazon SageMaker provides and maintains MXNet containers natively. However, in cases like the one in this lab, we may want to install or upgrade select depedencies. You have the option of creating your own SageMaker compatible container and use the MXNet container as a base, or update the dependencies at launch time using [requirements.txt](https://pip.pypa.io/en/stable/user_guide/#id15).\n",
    "\n",
    "DevOps best practices recommend building your own container. Requirements.txt is often ideal for development. You can apply minor updates quickly to support rapid prototyping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Run the cell below to preview the dependency updates that we'll run when we launch our SageMaker training and serving environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize $SOURCE_DIR/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Enabled Environment Considerations\n",
    "\n",
    "---\n",
    "Your environment and certain hyperparameters/configurations are dependent on the amount of available GPU. We need to ensure the right libraries are installed if you select a GPU enabled instance. \n",
    "\n",
    "Secondly, if you have multiple GPUs either on a single instance or distributed cluster, you have more GPU memory available to support larger batch sizes for accelerating training time. Larger batch sizes will help accelerate and scale your training process.\n",
    "\n",
    "This YOLOv3 script also needs to know how many devices you like to use for multiple GPU training on a single instance, so that data sharding can be done in the script to facilitate data parallelism scaling.\n",
    "\n",
    "Note that you may need to adapt to model performance degradation as a result of training on large batch sizes by lowering the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# sample selection of SageMaker instances and available GPUs\n",
    "INSTANCE_GPUS = {\n",
    "    'ml.p3.2xlarge':1,\n",
    "    'ml.p3.8xlarge':4,\n",
    "    'ml.c5.4xlarge':0,\n",
    "    'ml.c5.2xlarge':0,\n",
    "    'ml.c5.xlarge':0,\n",
    "    'ml.g4dn.xlarge':1\n",
    "}\n",
    "\n",
    "def set_requirements_text(gpu_train=True, framework_v=FRAMEWORK_VERSION) :\n",
    "    \n",
    "#    if gpu_train :\n",
    "#        shutil.copy('../scripts/requirements-gpu.txt','../scripts/requirements.txt')\n",
    "#    else :\n",
    "#        shutil.copy('../scripts/requirements-cpu.txt','../scripts/requirements.txt')\n",
    "\n",
    "    req_file = \"{}/requirements.txt\".format(SOURCE_DIR)\n",
    "    with open(req_file, \"r+\") as f:\n",
    "        data = f.read()\n",
    "        f.seek(0)\n",
    "        f.write(\"gluoncv=={}\".format(GLUONCV_COMPATIBILITY[framework_v]))\n",
    "        f.truncate()\n",
    "        \n",
    "    !pygmentize $SOURCE_DIR/requirements.txt\n",
    "\n",
    "\n",
    "def environment_device_config(instance_selection, acceleration_type=None, framework_v=FRAMEWORK_VERSION) :\n",
    "    \n",
    "    USE_GPU = (INSTANCE_GPUS[instance_selection] > 0)\n",
    "\n",
    "    gpu_devices = '0'\n",
    "    for i in range(1,INSTANCE_GPUS[instance_selection]) :\n",
    "        gpu_devices+= \",\"+str(i)\n",
    "\n",
    "    print('Set requirements.txt for {}:'.format('GPU' if USE_GPU else 'CPU')) \n",
    "    print('-----------------------------------')\n",
    "    set_requirements_text(USE_GPU,framework_v)\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    config={\n",
    "        \"gpu_devices\":gpu_devices,\n",
    "        \"batch_size_range\": [1*INSTANCE_GPUS[instance_selection] if USE_GPU else 16,\n",
    "                          16*INSTANCE_GPUS[instance_selection] if USE_GPU else 32]\n",
    "    }\n",
    "    \n",
    "    print('You have selected {}{}, which will utilize {}.'.format(instance_selection, \n",
    "                                                                '' if acceleration_type == None else \" with EIA ({})\".format(acceleration_type),\n",
    "                                                                'GPU' if USE_GPU else 'CPU' if acceleration_type == None else \"EIA\"))\n",
    "    \n",
    "    print('Recommended configurations: {}'.format(config))\n",
    "    \n",
    "    return config\n",
    "\n",
    "LOCAL_INSTANCE_TYPE = 'ml.g4dn.xlarge'\n",
    "config = environment_device_config(LOCAL_INSTANCE_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Amazon SageMaker Local\n",
    "\n",
    "We're now ready to configure our script with appropriate hyperparameters, and launch Amazon SageMaker training in local mode. \n",
    "\n",
    "We'll use the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.0001,\n",
    "    'epochs': 1,\n",
    "    'batch-size': np.floor((config['batch_size_range'][0]+config['batch_size_range'][1])/2),\n",
    "    'network':'mobilenet1.0',\n",
    "    'syncbn':True,\n",
    "    'num-workers':1,\n",
    "    'save-format':'symbolic',\n",
    "    'pre-trained-weights':\"VOC\"} \n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample dataset of the Caltech Birds dataset in RecordIO format should have been copied over to your notebook through the Github integration. Run the cell to validate the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "LOCAL_TRAIN_DATA_DIR = os.path.join(os.getcwd(),'../data/caltech_birds_sample/recordio/train')\n",
    "LOCAL_TEST_DATA_DIR = os.path.join(os.getcwd(),'../data/caltech_birds_sample/recordio/test')\n",
    "\n",
    "print('Local training data directory contents:')\n",
    "print('---------------------------------------')\n",
    "for f in listdir(LOCAL_TRAIN_DATA_DIR) :\n",
    "    print(f)\n",
    "print()\n",
    "print('Local test data directory contents:')\n",
    "print('---------------------------------------')\n",
    "for f in listdir(LOCAL_TEST_DATA_DIR) :\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate an MXNet estimator. This object ecapsulates your MXNet algorithm including infrastructure configurations that it will leverage when you execute training via the fit() method.\n",
    "\n",
    "The parameter **train_instance_type = 'local'** informs SageMaker that you wish to run this estimator in local mode. Secondly, ensure you replace /'<<NAME YOUR TRAINING JOB/>>' with a unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install 'sagemaker[local]' --upgrade\n",
    "\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "local_session = LocalSession()\n",
    "local_session.config = {'local': {'local_code': True}}\n",
    "\n",
    "#BASE_JOB_NAME ='dylantong-yolo3-local-training'\n",
    "BASE_JOB_NAME='<<NAME YOUR TRAINING JOB>>'\n",
    "\n",
    "local_estimator = MXNet(entry_point='gluoncv_yolo3-train.py',\n",
    "                        role=role,\n",
    "                        sagemaker_session = local_session,\n",
    "                        source_dir=SOURCE_DIR,\n",
    "                        framework_version=FRAMEWORK_VERSION,\n",
    "                        py_version='py3',\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='local',\n",
    "                        hyperparameters=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fit() will deploy a training container locally to run your training script. In local mode, you have the option of utilizing data from a local file system as shown below. Alternatively, you can specify S3 locations as would normally do for remote training (commented out below).\n",
    "\n",
    "**SageMaker local mode doesn't work with in SageMaker Studio at this time 8/27/2020. Please skip this cell for now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LOCAL_TRAIN_INPUT = local_session.upload_data(path=LOCAL_TRAIN_DATA_DIR)\n",
    "LOCAL_TEST_INPUT = local_session.upload_data(path=LOCAL_TEST_DATA_DIR)\n",
    "\n",
    "local_estimator.fit({\"train\":LOCAL_TRAIN_INPUT, \"test\":LOCAL_TEST_INPUT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Train the model\n",
    "\n",
    "So you've identified candidate algorithm(s) and an architecture that is ready for larger scale training, and you've validated your script in a SageMaker training environment via SageMaker Local. \n",
    "\n",
    "You can now leverage Amazon SageMaker to perform large-scale training with zero setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define your hyperparameters\n",
    "\n",
    "After you've verified that your script runs on Amazon SageMaker training without errors, a good first step is utilize the SageMaker's Automatic Model Tuning to hone in a good set of hyperparameters.\n",
    "\n",
    "Let's prepare configurations for GPU training so our tuning job can get done in a reasonable time. With SageMaker Training you only pay for the resources during training. GPU instances are more expensive, but they can also reduce training time by magnitudes compared to training on CPU, which in turn reduces training time costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTANCE_SELECTION = 'ml.p3.8xlarge'\n",
    "config = environment_device_config(INSTANCE_SELECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we need to specify the objective metric so that SageMaker can use Bayesian Optimization to find hyperparameters that optimize on a specific objective. We'll use the standard objective, which is to maximize mAP on the validation set.\n",
    "\n",
    "The regex experession informs SageMaker how to parse the training logs to obtain the configured objective metric. Your script should have logic to write out the objective metric into the logs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTIVE_METRIC_NAME = 'mAP'\n",
    "OBJECTIVE_TYPE = 'Maximize'\n",
    "\n",
    "METRIC_DEFINITIONS = [{ 'Name':OBJECTIVE_METRIC_NAME,\n",
    "                        'Regex':'Target Objective: {} = ([0-9\\\\.]+)'.format(OBJECTIVE_METRIC_NAME)}]\n",
    "\n",
    "print(METRIC_DEFINITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, we set the hyperparameters as well as the the hyperparameter value ranges that you want Amazon SageMaker to search over.\n",
    "\n",
    "The configurations below will search for optimal values for the optimizer to use, the learning rate, and the mini-batch-size. In practice, you might want to explore other variables such as the base network, pretrained weights, and other variables that could influence the performance of your model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETER_RANGES = {'opt': CategoricalParameter(['adam','sgd']),\n",
    "                         'lr': ContinuousParameter(0.0002, 0.001, scaling_type='Logarithmic'),\n",
    "                         'batch_size': IntegerParameter(config['batch_size_range'][0], \n",
    "                                                        config['batch_size_range'][1])}\n",
    "params = {\n",
    "    'opt': 'adam',\n",
    "    'epochs': 1,\n",
    "    'network':'mobilenet1.0',\n",
    "    'syncbn':True,\n",
    "    'gpus': config['gpu_devices'] \n",
    "} \n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Launch your Hyperparameter Tuning Job\n",
    "\n",
    "Similar to Lab 2, we're going to launch an hyperparamter job to automate the hyperparameter search process. The difference is that we're launching this job programatically via the Python SDK. \n",
    "\n",
    "Secondly, since we're training on a custom script, we have a couple of additional parameters: (1) we specify the location of the scripts in our local directory (source_dir) (2) we provide the name of our MXNet (GluonCV) script that serves as an entry point (entry_point) for our algorithm. \n",
    "\n",
    "Lastly, we have provided a requirements.txt file in the same directory as our scripts, so that it will be loaded into our training containers. Requirements.txt will instruct SageMaker to install the necessary dependencies on to the training containers upon launch. The values framework_version='1.3.0' and py_version='py3' informs SageMaker to use the MXNet container that has been pre-built and tested for MXNet1.3 running on python3.\n",
    "\n",
    "Replace '\\<<NAME YOUR HPO JOB\\>>' with a unique name, so that you can identify your hyperparameter tuning jobs.\n",
    "\n",
    "In this cell we instantiate an MXNet estimator. This object ecapsulates your MXNet algorithm including infrastructure configurations that it will leverage when you execute training via the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_JOB_NAME='dylantong-yolo3-hpo'\n",
    "BASE_JOB_NAME='<<NAME YOUR HPO JOB>>'\n",
    "\n",
    "estimator = MXNet(entry_point='gluoncv_yolo3-train.py',\n",
    "                  role=role,\n",
    "                  source_dir=SOURCE_DIR,\n",
    "                  framework_version=FRAMEWORK_VERSION,\n",
    "                  py_version='py3',\n",
    "                  train_instance_count=1,\n",
    "                  train_instance_type=INSTANCE_SELECTION,\n",
    "                  sagemaker_session=sagemaker.Session(),\n",
    "                  base_job_name=BASE_JOB_NAME,\n",
    "                  hyperparameters=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the parameters for our automatic hyperparameter tuner job. We provide the same parameters as we did in Lab 2 except we're instantiating the job programmatically via the SDK. Note that we only run two jobs (max_jobs=2) for the sake of demonstration. In practice, you'll need to run more to hone in on optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            OBJECTIVE_METRIC_NAME,\n",
    "                            HYPERPARAMETER_RANGES,\n",
    "                            METRIC_DEFINITIONS,\n",
    "                            objective_type = OBJECTIVE_TYPE,\n",
    "                            max_jobs=1,\n",
    "                            max_parallel_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fit() on our tuner launch the job as a non-blocking asynchronous call. We pass two parameters to specify the location of our training and test sets.\n",
    "\n",
    "'Train' and 'Test' will be treated as \"channels\" and the training container will have environment variables with the name SM_CHANNEL_TRAIN and SM_CHANNEL_TEST created respectively. The datasets will be loaded from the specified S3 location, and can be access in the training container context via the environment variables (eg. os.environ['SM_CHANNEL_TRAIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': SAMPLE_TRAINING_DATASET, \n",
    "           'test': SAMPLE_TEST_DATASET})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker's Automatic Model tuning functionality can run tuning jobs in sequence and in parallel asynchronously. You can use the .wait() command to block a thread and wait for completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Finetune your model\n",
    "\n",
    "The training process will be iterative as described in Lab 2. Eventually, you will hone in on a set of parameters, and may choose to further fine tune the model.\n",
    "\n",
    "The section below demonstrates how to perform incremental training on a model that you've previously trained. For instance, this could be the best model from your recent Hyperparameter Tuning Job. \n",
    "\n",
    "The training time required to train a YOLOv3 model is beyond what is practical for a workshop. Therefore, I've provided a pretrained YOLOv3 model that has been finetuned a sample of our CalTech Birds dataset. You can treat this model as if it were the output of your Hyperparameter Tuning Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL = 'yolo3_mobilenet1.0_custom_best.params'\n",
    "TRAINED_MODEL_URI = 's3://reinvent2018-sagemaker-pytorch/models/caltech-birds/gluoncv/yolov3/sample/{}'.format(TRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select an instance and rest our parameters. We'll use a smaller GPU instance for the purpose of minimizing the cost of this lab.\n",
    "\n",
    "Note that we've also set the epochs to one for the sake of demonstration. The provided model has been trained for hundreds of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INSTANCE_SELECTION = 'ml.p3.2xlarge'\n",
    "config = environment_device_config(INSTANCE_SELECTION)\n",
    "\n",
    "params = {\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.0001,\n",
    "    'epochs': 1,\n",
    "    'batch-size': config['batch_size_range'][1],\n",
    "    'network':'mobilenet1.0',\n",
    "    'syncbn':True,\n",
    "    'pre-trained-weights':\"NONE\",\n",
    "    'resume': TRAINED_MODEL,\n",
    "    'save-format': 'symbolic',\n",
    "    'gpus': config['gpu_devices'] } \n",
    "print()\n",
    "print(\"Configured Parameters\")\n",
    "print(\"---------------------\")\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace '\\<<NAME YOUR TRAINING JOB\\>>' with an unique prefix that will help you identify your training job.\n",
    "\n",
    "We're going to instantiate another MXNet estimator object. The main difference is the \"model_uri\" parameter, which will result in having the model artifacts at the specified S3 location to be uploaded into the training container. Your script can then initialize a model with those weights. The artifacts are located in the directory specified by the SM_CHANNEL_MODEL environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BASE_JOB_NAME ='dylantong-yolo3-incremental-training'\n",
    "BASE_JOB_NAME='<<NAME YOUR TRAINING JOB>>'\n",
    "\n",
    "incr_estimator = MXNet(entry_point='gluoncv_yolo3-train.py',\n",
    "                       role=role,\n",
    "                       source_dir=SOURCE_DIR,\n",
    "                       framework_version=FRAMEWORK_VERSION,\n",
    "                       py_version='py3',\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type=INSTANCE_SELECTION,\n",
    "                       sagemaker_session=sagemaker.Session(),\n",
    "                       base_job_name=BASE_JOB_NAME,\n",
    "                       hyperparameters=params,\n",
    "                       model_uri=TRAINED_MODEL_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incr_estimator.fit({'train': SAMPLE_TRAINING_DATASET, \n",
    "                    'test': SAMPLE_TEST_DATASET})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to programatically deploy our model, so that we have a managed endpoint for inference. \n",
    "\n",
    "This requires us to provide a script that is capable of loading a model into a model serving container and responding to requests. SageMaker expects integrators to implement a programmatic [interface](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#model-serving). Refer to the [link](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#model-serving) for details. The interface is common among all the BYOS integrations (eg. Tensorflow, PyTorch, MXNet).\n",
    "\n",
    "The interface has default implementations, but generally, you will need to provide your own implementation. Run the cell below to view the model serving script that has been provided for the YOLO3 model. The model expects images in a binary format, and returns three tensors (class, scores, bounding box) as a byte stream in a JSON response. \n",
    "\n",
    "You have to freedom to accept and respond with different formats, and there are certainly performance improvements that can be made with the current implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize $SOURCE_DIR/gluoncv-yolo3-inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by selecting our instance, and ensuring the right dependencies will be installed on our model serving instance. We'll use a c5, a CPU optimize instance, but attach GPU to the instance via Elastic Inference Acceleration (EIA). GPU should provide us better throughput on YOLOv3. With EIA, you'll get performance that betters the K80 GPUs that are available on the P2 instances at a lower cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8/28/2020 MXNET 1.6 containers don't have support for elastic inference. Using MXNet 1.5.1 for serving temporarily.\n",
    "INFERENCE_MXNET_VER = '1.5.1'\n",
    "ACCELERATION_TYPE = 'ml.eia1.medium'\n",
    "#INFERENCE_MXNET_VER = '1.6.0'\n",
    "#ACCELERATION_TYPE = None\n",
    "\n",
    "INSTANCE_SELECTION = 'ml.c5.xlarge'\n",
    "#INSTANCE_SELECTION = 'ml.p3.2xlarge'\n",
    "\n",
    "USE_GPU = INSTANCE_GPUS[INSTANCE_SELECTION] > 0\n",
    "USE_EIA = (ACCELERATION_TYPE is not None) and not USE_GPU\n",
    "IMG_WIDTH = 512\n",
    "#IMG_WIDTH = 180\n",
    "\n",
    "config = environment_device_config(INSTANCE_SELECTION, ACCELERATION_TYPE, INFERENCE_MXNET_VER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's programatically register the last model that we trained, so that it is available for deployment within SageMaker Hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = {\"SAGEMAKER_MODEL_SERVER_TIMEOUT\":\"3600\",\n",
    "            \"SYM_FILE_NAME\":\"yolov3-gcv-symbol.json\",\n",
    "            \"PARAM_FILE_NAME\":\"yolov3-gcv-0000.params\",\n",
    "            \"USE_GPU\": str(int(USE_GPU)),\n",
    "            \"USE_EIA\": str(int(USE_EIA)),\n",
    "            \"IMG_WIDTH\": str(IMG_WIDTH)}\n",
    "                              \n",
    "\n",
    "yolo_model = MXNetModel(model_data = incr_estimator.model_data,\n",
    "                        role=role,\n",
    "                        framework_version=INFERENCE_MXNET_VER,\n",
    "                        py_version=\"py3\",\n",
    "                        entry_point='gluoncv-yolo3-inference.py',\n",
    "                        env = env_vars,\n",
    "                        source_dir=incr_estimator.source_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace \\'<<PROVIDE AN ENDPOINT NAME\\>>' with an unique name so that you can identify your endpoint. For the purpose of demonstration, we only deploy a single instance. However, the endpoint can be configured for auto-scaling and deployed across a fleet to support throughput requirements.\n",
    "\n",
    "**This may take around 10 minutes to deploy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RT_ENDPOINT_NAME = 'dylantong-yolov3-rt-ep-{}'.format(datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "RT_ENDPOINT_NAME = '<<PROVIDE AN ENDPOINT NAME>>'\n",
    "print('endpoint name: {}'.format(RT_ENDPOINT_NAME))\n",
    "\n",
    "start = time.time()\n",
    "yolo_rt_ep = yolo_model.deploy(instance_type=INSTANCE_SELECTION,\n",
    "                                initial_instance_count = 1,\n",
    "                                endpoint_name= RT_ENDPOINT_NAME,\n",
    "                                accelerator_type=ACCELERATION_TYPE)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Endpoint was deployed in {:3f} seconds.\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download some sample images from the internet, and see how well our bird detector does on detecting these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O multi-goldfinch-1.jpg https://t3.ftcdn.net/jpg/01/44/64/36/500_F_144643697_GJRUBtGc55KYSMpyg1Kucb9yJzvMQooW.jpg\n",
    "!wget -q -O northern-flicker-1.jpg https://upload.wikimedia.org/wikipedia/commons/5/5c/Northern_Flicker_%28Red-shafted%29.jpg\n",
    "!wget -q -O northern-cardinal-1.jpg https://cdn.pixabay.com/photo/2013/03/19/04/42/bird-94957_960_720.jpg\n",
    "!wget -q -O blue-jay-1.jpg https://cdn12.picryl.com/photo/2016/12/31/blue-jay-bird-feather-animals-b8ee04-1024.jpg\n",
    "!wget -q -O hummingbird-1.jpg http://res.freestockphotos.biz/pictures/17/17875-hummingbird-close-up-pv.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our endpoint expects the request payload to be an image sent as bytes within the request body. Here's a utility function to generate the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_request_payload(img_loc) :\n",
    "\n",
    "    with open(img_loc, 'rb') as image:\n",
    "        f = image.read()\n",
    "        b = bytearray(f)\n",
    "        \n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model server implementation returns responses in JSON format. Here's a utility function that retrieves the bytes from the JSON payload, and restructures them back into tensor outputs corresponding to the class, score and bounding box predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CID_END_OFFSET = 100\n",
    "SCORE_END_OFFSET = 200\n",
    "BBOX_END_OFFSET = 600\n",
    "Y = 100\n",
    "N_SCORES = 1\n",
    "N_CID = 1\n",
    "N_BBOX_POINTS = 4\n",
    "\n",
    "def parse_response_payload(resp) :\n",
    "    \n",
    "    stacked_tensors = np.frombuffer(resp['Body'].read(), dtype=\"float32\")\n",
    "    #stacked_tensors = np.frombuffer(resp, dtype=\"float32\")\n",
    "    t= np.split(stacked_tensors,[CID_END_OFFSET,SCORE_END_OFFSET,BBOX_END_OFFSET])\n",
    "\n",
    "    c= mx.nd.array(t[0].reshape(Y,N_SCORES))\n",
    "    s= mx.nd.array(t[1].reshape(Y,N_CID))\n",
    "    bb= mx.nd.array(t[2].reshape(Y,N_BBOX_POINTS))\n",
    "    \n",
    "    return c,s,bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to detect the birds on the collection of images that we've downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_IMAGES = ['multi-goldfinch-1.jpg','northern-flicker-1.jpg', \n",
    "          'northern-cardinal-1.jpg','blue-jay-1.jpg','hummingbird-1.jpg']\n",
    "CLASSES = ['Cardinal','Northern_Flicker','American_Goldfinch', 'Ruby_throated_Hummingbird','Blue_Jay']\n",
    "runtime = boto3.client(service_name='runtime.sagemaker')\n",
    "\n",
    "def test_endpoint(img_file_list) :\n",
    "    \n",
    "    for img in img_file_list :\n",
    "\n",
    "        b = generate_request_payload(img)\n",
    "        \n",
    "        response = runtime.invoke_endpoint(EndpointName= yolo_rt_ep.endpoint,\n",
    "                                            ContentType='image/jpeg',\n",
    "                                            Body=b)\n",
    "\n",
    "        c,s,bb = parse_response_payload(response)\n",
    "\n",
    "        trans_X, trans_image = gcv.data.transforms.presets.yolo.transform_test(mx.img.imdecode(b), IMG_WIDTH)\n",
    "        ax = viz.plot_bbox(trans_image, bb, s,c, class_names=CLASSES)\n",
    "        plt.show()\n",
    "    \n",
    "test_endpoint(TEST_IMAGES)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "Here we delete the SageMaker endpoint, as we will no longer be performing any inferences.  This is an important step, as your account is billed for the amount of time an endpoint is running, even when it is idle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(yolo_rt_ep.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (MXNet GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/mxnet-1.6-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
