{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using GluonCV on Amazon SageMaker\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Data Preparation and Exploration](#[Optional]-Data-Preparation-and-Exploration)\n",
    "4. [Experimentation and Prototyping](#Experimentation-and-Prototyping)\n",
    "5. [Train the model](#Train-the-model)\n",
    "6. [Host the model](#Host-the-model)\n",
    "7. [Test the model](#Test-the-model)\n",
    "8. [Clean up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Object detection is the process of identifying and localizing objects in an image. A typical object detection solution takes an image as input and provides a bounding box on the image where an object of interest is found.  It also identifies what type of object the box encapsulates.  To create such a solution, we need to acquire and process a traning dataset, create and setup a training job for the alorithm so that it can learn about the dataset. Finally, we can then host the trained model in an endpoint, to which we can supply images.\n",
    "\n",
    "This notebook is an end-to-end example showing how GluonCV's Object Detection algorithms can be used with Amazon SageMaker, and a public dataset of bird images. We demonstrate how to train and to host an object detection model based on the [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset.  GluonCV provides variants of SSD, YOLOv3 and Faster RCNN algorithms for object detection algorithm. In this notebook we demonstrate YOLOv3 on mobilenet1.0 with weights pretrained on the [VOC](http://host.robots.ox.ac.uk/pascal/VOC/) dataset.\n",
    "\n",
    "![Sample results detecting a pair of goldfinch on a feeder](./goldfinch_detections.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before preparing the data, there are some initial steps required for setup.\n",
    "\n",
    "First, you should be running this notebook on a **ml.c5d.4xlarge** or larger instance if you plan to run the training script locally on your notebook. Otherwise, you'll need to lower the batch size to avoid out-of-memory errors.\n",
    "\n",
    "Any GPU instances like the ml.p3.2xlarge will do, and in practice, will be the ideal choice for a notebook instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to import the depedencies on this notebook. Most of the libraries that you need for MXNet and Gluon development are already packaged into the conda_mxnet_p36 environment.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We'll need to install some dependencies that might not be included in your MXNet environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached https://files.pythonhosted.org/packages/5a/4a/39400ff9b36e719bdf8f31c99fe1fa7842a42fa77432e584f707a5080063/pip-20.2.2-py2.py3-none-any.whl\n",
      "Installing collected packages: pip\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "Successfully installed pip-20.2.2\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.3.1-cp36-cp36m-manylinux1_x86_64.whl (11.6 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/site-packages (from matplotlib) (1.17.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.6/site-packages (from matplotlib) (6.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib) (2.4.6)\n",
      "Collecting certifi>=2020.06.20\n",
      "  Using cached certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/site-packages (from python-dateutil>=2.1->matplotlib) (1.14.0)\n",
      "Installing collected packages: kiwisolver, certifi, cycler, matplotlib\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2019.11.28\n",
      "    Uninstalling certifi-2019.11.28:\n",
      "      Successfully uninstalled certifi-2019.11.28\n",
      "Successfully installed certifi-2020.6.20 cycler-0.10.0 kiwisolver-1.2.0 matplotlib-3.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.mxnet import MXNet, MXNetModel\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GluonCV isn't pre-installed into this environment. However, most of the dependencies required by GluonCV have already been packaged into the **conda_mxnet_p36** environment. \n",
    "\n",
    "This notebook has requirements that we will have to install manually:\n",
    "* MXNet 1.6+\n",
    "* GluonCV libraries\n",
    "\n",
    "Installation instructions for GluonCV are provided [here](https://gluon-cv.mxnet.io/#installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MXNet dependencies are GPU dependent, so we'll check for the presence of GPU devnices first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device is available\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "try:\n",
    "    a = mx.nd.zeros((1,), ctx=mx.gpu(0))\n",
    "    ctx = [mx.gpu(0)]\n",
    "    print('GPU device is available')\n",
    "except:\n",
    "    ctx = [mx.cpu()]\n",
    "    USE_GPU = False\n",
    "    print('Using CPU on local machine. GPU device was not detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we...\n",
    "* Upgrade MXNet 1.6 for CUDA 10.1 with the appropriate libraries depending on whether GPU is available.\n",
    "* Install GluonCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing MXNet: pip install mxnet-cu101mkl==1.6.0 --upgrade\n",
      "Install GluonCV: pip install gluoncv==0.8.0 --upgrade\n",
      "GluonCV dependencies were installed in 2.570071 seconds.\n"
     ]
    }
   ],
   "source": [
    "FRAMEWORK_VERSION = \"1.6.0\"\n",
    "GLUONCV_COMPATIBILITY = {\n",
    "    \"1.5.0\":\"0.6.0\",\n",
    "    \"1.6.0\":\"0.8.0\"\n",
    "}\n",
    "start = time.time()\n",
    "\n",
    "if USE_GPU :\n",
    "    cmd = 'pip install mxnet-cu101mkl=={} --upgrade'.format(FRAMEWORK_VERSION)\n",
    "else :\n",
    "    cmd ='pip install mxnet-mkl=={} --upgrade'.format(FRAMEWORK_VERSION)\n",
    "\n",
    "print(\"Changing MXNet: {}\".format(cmd))\n",
    "os.system(cmd)\n",
    "\n",
    "cmd = \"pip install gluoncv=={} --upgrade\".format(GLUONCV_COMPATIBILITY[FRAMEWORK_VERSION])\n",
    "print(\"Install GluonCV: {}\".format(cmd))\n",
    "os.system(cmd)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"GluonCV dependencies were installed in {:3f} seconds.\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gluoncv should now be available for us. Import our required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluoncv as gcv\n",
    "from gluoncv.utils import download, viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next session, we'll create an optimized dataset (RecordIO format) for training our YOLOv3 model. The process can take 15 minutes. \n",
    "\n",
    "It's up to you whether you like to go through the steps of pre-processing your dataset into RecordIO format. Alternatively, I've provided pre-processed datasets for you to use.\n",
    "\n",
    "Run the cell below to set the locations of the pre-processed train and validation datasets. Note that MXNet doesn't require RecordIO format, but there are various benefits as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TRAINING_DATASET = 's3://reinvent2018-sagemaker-pytorch/datasets/caltech-birds/recordio/train'\n",
    "SAMPLE_TEST_DATASET = 's3://reinvent2018-sagemaker-pytorch/datasets/caltech-birds/recordio/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] Data Preparation and Exploration\n",
    "\n",
    "---\n",
    "This section is **optional**. For the purpose of making this lab as efficient as possible, data sets have already been prepared for you in MXNet [RecordIO format](https://mxnet.incubator.apache.org/versions/master/faq/recordio.html), which has various [benefits](https://mxnet.incubator.apache.org/versions/master/faq/recordio.html) including performance enhancements. The following are steps that were taken to produce training and validation samples in RecordIO format. Take note of the utility functions that MXNet provides for format conversion as well as the native data loaders. These are great features that reduce data wrangling work, and aren't provided by most frameworks.\n",
    "\n",
    "---\n",
    "This [link](https://cv-workshop-2.notebook.us-west-2.sagemaker.aws/notebooks/aws-cv-jumpstarter/notebooks/lab3b-optional-generate-recordio-files.ipynb) will launch another notebook, which will guide you through this optional section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation and Prototyping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook environment is an ideal environment for rapid prototyping and experimentation. Run the cell below to preview the YOLOv3 training script that we will use to train the model.\n",
    "\n",
    "The script is no different than one you would run outside of the Amazon SageMaker environment. The only exception is that are conditions in the script to set parameters to environment variables that will be present when we run remote SageMaker training. They're only necessary if you plan to refactor your script to take advantage of the zero-setup SageMaker training capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mwarnings\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmx\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m nd\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gluon\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m autograd\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mgcv\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m data \u001b[34mas\u001b[39;49;00m gdata\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m utils \u001b[34mas\u001b[39;49;00m gutils\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodel_zoo\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_model\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mbatchify\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Tuple, Stack, Pad\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransforms\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpresets\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36myolo\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m YOLO3DefaultTrainTransform\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtransforms\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mpresets\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36myolo\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m YOLO3DefaultValTransform\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdataloader\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m RandomTransformDataLoader\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mvoc_detection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m VOC07MApMetric\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mcoco_detection\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m COCODetectionMetric\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LRScheduler, LRSequential\n",
      "\n",
      "os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mMXNET_CUDNN_AUTOTUNE_DEFAULT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[33m'\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "SYM_MODEL_NAME= \u001b[33m\"\u001b[39;49;00m\u001b[33myolov3-gcv\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "EVAL_METRICS = {\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mmAP\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[34m0.0\u001b[39;49;00m,\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mObjLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[34m10000.0\u001b[39;49;00m,\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mBoxCenterLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[34m10000.0\u001b[39;49;00m,\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mBoxScaleLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[34m10000.0\u001b[39;49;00m,\n",
      "    \u001b[33m'\u001b[39;49;00m\u001b[33mClassLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\u001b[34m10000.0\u001b[39;49;00m\n",
      "}\n",
      "\n",
      "MODEL_DIR = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "CLASSES_FILE = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser = argparse.ArgumentParser(description=\u001b[33m'\u001b[39;49;00m\u001b[33mTrain YOLOv3 networks with random input shape.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--network\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mdarknet53\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mBase network name which serves as feature extraction base.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--obj-metric\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mmAP\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mSelect an objective metric to optimize using a SageMaker tuner: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mmAP\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mObjective-Loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mBox-Center-Loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mBox-Scale-Loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mClass-Loss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-shape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m416\u001b[39;49;00m,\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mInput data shape for evaluation, use 320, 416, 608... \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m +\n",
      "                             \u001b[33m\"\u001b[39;49;00m\u001b[33mTraining is with random shapes from (320 to 608).\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--opt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m\"\u001b[39;49;00m\u001b[33mOptimzer to use for training. Currently only sgd or adam is supported.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mTraining mini-batch size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--pre-trained-weights\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mVOC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSet COCO or VOC pre-trained weights. Otherwise, set NONE to disable pre-training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--data-prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mImage folder prefix relative to train or test channel as referenced by LST files.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)                   \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m-j\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dest=\u001b[33m'\u001b[39;49;00m\u001b[33mnum_workers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\n",
      "                        default=\u001b[34m4\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mNumber of data workers, you can use larger \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mnumber to accelerate data loading, if you CPU and GPUs are powerful.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mTraining with GPUs, you can specify 1,3 for example.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m200\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mTraining epochs.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--resume\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mResume from previously saved parameters if not None. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mFor example, you can resume from ./yolo3_xxx_0123.params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--start-epoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mStarting epoch for resuming, default is 0 for new training.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mYou can specify it to 100 for example to start from 100 epoch.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.001\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mLearning rate, default is 0.001\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr-mode\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33mstep\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mlearning rate scheduler mode. options are step, poly and cosine.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr-decay\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.1\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mdecay rate of learning rate. default is 0.1.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr-decay-period\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33minterval for periodic learning rate decays. default is 0 to disable.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--lr-decay-epoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m160,180\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mepochs at which learning rate decays. default is 160,180.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--warmup-lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.0\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mstarting warmup learning rate. default is 0.0.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--warmup-epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mnumber of warmup epochs.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--momentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.9\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSGD momentum, default is 0.9\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--wd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.0005\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mWeight decay, default is 5e-4\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m100\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mLogging mini-batch interval. Default is 100.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save-prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSaving parameter prefix\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSaving parameters epoch interval, best model will always be saved.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--val-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mEpoch interval for validation, increase the number will reduce the \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                             \u001b[33m'\u001b[39;49;00m\u001b[33mtraining time if validation is slow.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--seed\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m233\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mRandom seed to be fixed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num-samples\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=-\u001b[34m1\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mTraining images. Use -1 to automatically get the number.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--syncbn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mUse synchronize BN across devices.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--no-random-shape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mUse fixed size(data-shape) throughout the training, which will be faster \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "                        \u001b[33m'\u001b[39;49;00m\u001b[33mand require less memory. However, final model will be slightly worse.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--no-wd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mwhether to remove weight decay on bias, and beta/gamma for batchnorm layers.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--mixup\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mwhether to enable mixup.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--no-mixup-epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m20\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mDisable mixup training if enabled in the last N epochs.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--label-smooth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mUse label smoothing.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--local\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, action=\u001b[33m'\u001b[39;49;00m\u001b[33mstore_true\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, default=\u001b[34mFalse\u001b[39;49;00m,\n",
      "                        help=\u001b[33m'\u001b[39;49;00m\u001b[33mSet to true if you want to run this script locally.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--save-format\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m'\u001b[39;49;00m\u001b[33msymbolic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, help=\u001b[33m'\u001b[39;49;00m\u001b[33mSelect between imperative or symbolic as the save format\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    \u001b[37m# input data and model directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    \n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m args.local != \u001b[34mTrue\u001b[39;49;00m :\n",
      "        \u001b[34mif\u001b[39;49;00m args.model_dir == \u001b[34mNone\u001b[39;49;00m : args.model_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]    \n",
      "        \u001b[34mif\u001b[39;49;00m args.train == \u001b[34mNone\u001b[39;49;00m : args.train = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        \u001b[34mif\u001b[39;49;00m args.test == \u001b[34mNone\u001b[39;49;00m : args.test = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "\n",
      "        \u001b[34mtry\u001b[39;49;00m :\n",
      "            \u001b[34mif\u001b[39;49;00m os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_MODEL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] != \u001b[34mNone\u001b[39;49;00m :\n",
      "                \u001b[34mglobal\u001b[39;49;00m MODEL_DIR\n",
      "                MODEL_DIR = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_MODEL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "                args.resume = os.path.join(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_MODEL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],args.resume.strip())\n",
      "        \u001b[34mexcept\u001b[39;49;00m :\n",
      "                args.resume = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    \n",
      "    \u001b[34mglobal\u001b[39;49;00m CLASSES_FILE\n",
      "    CLASSES_FILE = os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mclasses.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \n",
      "    \u001b[34mreturn\u001b[39;49;00m args\n",
      "\n",
      "\u001b[37m# This script is only implemented to support RecordIO format. In addition to RecordIO data in\u001b[39;49;00m\n",
      "\u001b[37m# the train and test channel, the script expects the following files at the root:\u001b[39;49;00m\n",
      "\u001b[37m#   1. List files for training and validation sets named train.lst and val.lst at the root of the train and \u001b[39;49;00m\n",
      "\u001b[37m#      test channels respectively: \u001b[39;49;00m\n",
      "\u001b[37m#      (https://mxnet.incubator.apache.org/versions/master/faq/recordio.html#step-1-make-an-image-list-file)\u001b[39;49;00m\n",
      "\u001b[37m#   2. classes.csv which should be a comma-delimited list of two columns consisting of the class ids and \u001b[39;49;00m\n",
      "\u001b[37m#      names. This file should be located at the root of the train channel.\u001b[39;49;00m\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_data\u001b[39;49;00m():\n",
      "\n",
      "    train_dataset = gcv.data.RecordFileDetection(os.path.join(args.train,\u001b[33m'\u001b[39;49;00m\u001b[33mtrain.rec\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    val_dataset = gcv.data.RecordFileDetection(os.path.join(args.test,\u001b[33m'\u001b[39;49;00m\u001b[33mval.rec\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "\n",
      "    classes_df = pd.read_csv(CLASSES_FILE, header=\u001b[34mNone\u001b[39;49;00m)\n",
      "    classes = classes_df[\u001b[34m1\u001b[39;49;00m].tolist()\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.num_samples < \u001b[34m0\u001b[39;49;00m:\n",
      "        args.num_samples = \u001b[36mlen\u001b[39;49;00m(train_dataset)\n",
      "    \u001b[34mif\u001b[39;49;00m args.mixup:\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MixupDetection\n",
      "        train_dataset = MixupDetection(train_dataset)\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_dataset, val_dataset, classes\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_eval_metric\u001b[39;49;00m(args, CLASSES=\u001b[34mNone\u001b[39;49;00m, IOU_THRESH=\u001b[34m0.5\u001b[39;49;00m):\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.pre_trained_weights == \u001b[33m'\u001b[39;49;00m\u001b[33mVOC\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "        eval_metric = VOC07MApMetric(iou_thresh=IOU_THRESH, class_names=CLASSES)\n",
      "    \u001b[34melif\u001b[39;49;00m args.pre_trained_weights == \u001b[33m'\u001b[39;49;00m\u001b[33mCOCO\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "        eval_metric = COCODetectionMetric(\n",
      "            val_dataset, args.save_prefix + \u001b[33m'\u001b[39;49;00m\u001b[33m_eval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, cleanup=\u001b[34mTrue\u001b[39;49;00m,\n",
      "            data_shape=(args.data_shape, args.data_shape))\n",
      "    \u001b[34melif\u001b[39;49;00m args.pre_trained_weights == \u001b[33m'\u001b[39;49;00m\u001b[33mNONE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "        eval_metric = VOC07MApMetric(iou_thresh=IOU_THRESH, class_names=CLASSES)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mNotImplementedError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDataset: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m not implemented.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(dataset))\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m eval_metric\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_dataloader\u001b[39;49;00m(net, train_dataset, val_dataset, data_shape, batch_size, num_workers, args):\n",
      "    \u001b[33m\"\"\"Get dataloader.\"\"\"\u001b[39;49;00m\n",
      "    width, height = data_shape, data_shape\n",
      "    batchify_fn = Tuple(*([Stack() \u001b[34mfor\u001b[39;49;00m _ \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m6\u001b[39;49;00m)] + [Pad(axis=\u001b[34m0\u001b[39;49;00m, pad_val=-\u001b[34m1\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m _ \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m)]))  \u001b[37m# stack image, all targets generated\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.no_random_shape:\n",
      "        train_loader = gluon.data.DataLoader(\n",
      "            train_dataset.transform(YOLO3DefaultTrainTransform(width, height, net, mixup=args.mixup)),\n",
      "            batch_size, \u001b[34mTrue\u001b[39;49;00m, batchify_fn=batchify_fn, last_batch=\u001b[33m'\u001b[39;49;00m\u001b[33mrollover\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, num_workers=num_workers)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        transform_fns = [YOLO3DefaultTrainTransform(x * \u001b[34m32\u001b[39;49;00m, x * \u001b[34m32\u001b[39;49;00m, net, mixup=args.mixup) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m10\u001b[39;49;00m, \u001b[34m20\u001b[39;49;00m)]\n",
      "        train_loader = RandomTransformDataLoader(\n",
      "            transform_fns, train_dataset, batch_size=batch_size, interval=\u001b[34m10\u001b[39;49;00m, last_batch=\u001b[33m'\u001b[39;49;00m\u001b[33mrollover\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "            shuffle=\u001b[34mTrue\u001b[39;49;00m, batchify_fn=batchify_fn, num_workers=num_workers)\n",
      "    val_batchify_fn = Tuple(Stack(), Pad(pad_val=-\u001b[34m1\u001b[39;49;00m))\n",
      "    val_loader = gluon.data.DataLoader(\n",
      "        val_dataset.transform(YOLO3DefaultValTransform(width, height)),\n",
      "        batch_size, \u001b[34mFalse\u001b[39;49;00m, batchify_fn=val_batchify_fn, last_batch=\u001b[33m'\u001b[39;49;00m\u001b[33mkeep\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, num_workers=num_workers)\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_loader, val_loader\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_params\u001b[39;49;00m(net, best_map, current_map, epoch, args) :\n",
      " \n",
      "    current_map = \u001b[36mfloat\u001b[39;49;00m(current_map)\n",
      " \n",
      "    \u001b[34mif\u001b[39;49;00m current_map > best_map[\u001b[34m0\u001b[39;49;00m]:\n",
      "        \n",
      "        best_map[\u001b[34m0\u001b[39;49;00m] = current_map  \n",
      "        \n",
      "        \u001b[34mif\u001b[39;49;00m args.save_format == \u001b[33m'\u001b[39;49;00m\u001b[33mimperative\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "            net.save_parameters(\u001b[33m'\u001b[39;49;00m\u001b[33m{:s}\u001b[39;49;00m\u001b[33m_best.params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(os.path.join(args.model_dir,args.save_prefix)))\n",
      "            \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(prefix+\u001b[33m'\u001b[39;49;00m\u001b[33m_best_map.log\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ma\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\n",
      "                f.write(\u001b[33m'\u001b[39;49;00m\u001b[33m{:04d}\u001b[39;49;00m\u001b[33m:\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch, current_map))\n",
      "        \u001b[34melif\u001b[39;49;00m args.save_format == \u001b[33m'\u001b[39;49;00m\u001b[33msymbolic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "            net.export(\u001b[33m'\u001b[39;49;00m\u001b[33m{:s}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(os.path.join(args.model_dir,SYM_MODEL_NAME)))\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mUnsupported mode: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(args.mode))\n",
      "    \n",
      "    \u001b[34mif\u001b[39;49;00m args.save_interval \u001b[35mand\u001b[39;49;00m epoch % args.save_interval == \u001b[34m0\u001b[39;49;00m:\n",
      "        net.save_parameters(\u001b[33m'\u001b[39;49;00m\u001b[33m{:s}\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m{:04d}\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m{:.4f}\u001b[39;49;00m\u001b[33m.params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(os.path.join(args.model_dir,args.save_prefix), epoch, current_map))\n",
      "       \n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mvalidate\u001b[39;49;00m(net, val_data, ctx, eval_metric, obj_metric=\u001b[34m0\u001b[39;49;00m):\n",
      "    \u001b[33m\"\"\"Test on validation dataset.\"\"\"\u001b[39;49;00m\n",
      "    eval_metric.reset()\n",
      "    \u001b[37m# set nms threshold and topk constraint\u001b[39;49;00m\n",
      "    net.set_nms(nms_thresh=\u001b[34m0.45\u001b[39;49;00m, nms_topk=\u001b[34m400\u001b[39;49;00m)\n",
      "    mx.nd.waitall()\n",
      "    net.hybridize()\n",
      "    \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m val_data:\n",
      "        data = gluon.utils.split_and_load(batch[\u001b[34m0\u001b[39;49;00m], ctx_list=ctx, batch_axis=\u001b[34m0\u001b[39;49;00m, even_split=\u001b[34mFalse\u001b[39;49;00m)\n",
      "        label = gluon.utils.split_and_load(batch[\u001b[34m1\u001b[39;49;00m], ctx_list=ctx, batch_axis=\u001b[34m0\u001b[39;49;00m, even_split=\u001b[34mFalse\u001b[39;49;00m)\n",
      "        det_bboxes = []\n",
      "        det_ids = []\n",
      "        det_scores = []\n",
      "        gt_bboxes = []\n",
      "        gt_ids = []\n",
      "        gt_difficults = []\n",
      "        \u001b[34mfor\u001b[39;49;00m x, y \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(data, label):\n",
      "            \u001b[37m# get prediction results\u001b[39;49;00m\n",
      "            ids, scores, bboxes = net(x)\n",
      "            det_ids.append(ids)\n",
      "            det_scores.append(scores)\n",
      "            \u001b[37m# clip to image size\u001b[39;49;00m\n",
      "            det_bboxes.append(bboxes.clip(\u001b[34m0\u001b[39;49;00m, batch[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m2\u001b[39;49;00m]))\n",
      "            \u001b[37m# split ground truths\u001b[39;49;00m\n",
      "            gt_ids.append(y.slice_axis(axis=-\u001b[34m1\u001b[39;49;00m, begin=\u001b[34m4\u001b[39;49;00m, end=\u001b[34m5\u001b[39;49;00m))\n",
      "            gt_bboxes.append(y.slice_axis(axis=-\u001b[34m1\u001b[39;49;00m, begin=\u001b[34m0\u001b[39;49;00m, end=\u001b[34m4\u001b[39;49;00m))\n",
      "            gt_difficults.append(y.slice_axis(axis=-\u001b[34m1\u001b[39;49;00m, begin=\u001b[34m5\u001b[39;49;00m, end=\u001b[34m6\u001b[39;49;00m) \u001b[34mif\u001b[39;49;00m y.shape[-\u001b[34m1\u001b[39;49;00m] > \u001b[34m5\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# update metric\u001b[39;49;00m\n",
      "        eval_metric.update(det_bboxes, det_ids, det_scores, gt_bboxes, gt_ids, gt_difficults)\n",
      "    \u001b[34mreturn\u001b[39;49;00m eval_metric.get()\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(net, train_data, val_data, eval_metric, ctx, args):\n",
      "    \u001b[33m\"\"\"Training pipeline\"\"\"\u001b[39;49;00m\n",
      "    net.collect_params().reset_ctx(ctx)\n",
      "    \u001b[34mif\u001b[39;49;00m args.no_wd:\n",
      "        \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m net.collect_params(\u001b[33m'\u001b[39;49;00m\u001b[33m.*beta|.*gamma|.*bias\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).items():\n",
      "            v.wd_mult = \u001b[34m0.0\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.label_smooth:\n",
      "        net._target_generator._label_smooth = \u001b[34mTrue\u001b[39;49;00m\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.lr_decay_period > \u001b[34m0\u001b[39;49;00m:\n",
      "        lr_decay_epoch = \u001b[36mlist\u001b[39;49;00m(\u001b[36mrange\u001b[39;49;00m(args.lr_decay_period, args.epochs, args.lr_decay_period))\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        lr_decay_epoch = [\u001b[36mint\u001b[39;49;00m(i) \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m args.lr_decay_epoch.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)]\n",
      "    lr_decay_epoch = [e - args.warmup_epochs \u001b[34mfor\u001b[39;49;00m e \u001b[35min\u001b[39;49;00m lr_decay_epoch]\n",
      "    num_batches = args.num_samples // args.batch_size\n",
      "    lr_scheduler = LRSequential([\n",
      "        LRScheduler(\u001b[33m'\u001b[39;49;00m\u001b[33mlinear\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, base_lr=\u001b[34m0\u001b[39;49;00m, target_lr=args.lr,\n",
      "                    nepochs=args.warmup_epochs, iters_per_epoch=num_batches),\n",
      "        LRScheduler(args.lr_mode, base_lr=args.lr,\n",
      "                    nepochs=args.epochs - args.warmup_epochs,\n",
      "                    iters_per_epoch=num_batches,\n",
      "                    step_epoch=lr_decay_epoch,\n",
      "                    step_factor=args.lr_decay, power=\u001b[34m2\u001b[39;49;00m),\n",
      "    ])\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.opt == \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "        trainer = gluon.Trainer(\n",
      "            net.collect_params(), \u001b[33m'\u001b[39;49;00m\u001b[33msgd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "            {\u001b[33m'\u001b[39;49;00m\u001b[33mwd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.wd, \u001b[33m'\u001b[39;49;00m\u001b[33mmomentum\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.momentum, \u001b[33m'\u001b[39;49;00m\u001b[33mlr_scheduler\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: lr_scheduler},\n",
      "            kvstore=\u001b[33m'\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34melif\u001b[39;49;00m args.opt == \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "        trainer = gluon.Trainer(\n",
      "            net.collect_params(), \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "            {\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.lr, \u001b[33m'\u001b[39;49;00m\u001b[33mwd\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: args.wd})\n",
      "    \u001b[34melse\u001b[39;49;00m :\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mNotImplementedError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mOptimizer option: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m not implemented.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.opt))\n",
      "\n",
      "    \u001b[37m# targets\u001b[39;49;00m\n",
      "    sigmoid_ce = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=\u001b[34mFalse\u001b[39;49;00m)\n",
      "    l1_loss = gluon.loss.L1Loss()\n",
      "\n",
      "    \u001b[37m# metrics\u001b[39;49;00m\n",
      "    obj_metrics = mx.metric.Loss(\u001b[33m'\u001b[39;49;00m\u001b[33mObjLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    center_metrics = mx.metric.Loss(\u001b[33m'\u001b[39;49;00m\u001b[33mBoxCenterLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    scale_metrics = mx.metric.Loss(\u001b[33m'\u001b[39;49;00m\u001b[33mBoxScaleLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    cls_metrics = mx.metric.Loss(\u001b[33m'\u001b[39;49;00m\u001b[33mClassLoss\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# set up logger\u001b[39;49;00m\n",
      "    logging.basicConfig()\n",
      "    logger = logging.getLogger()\n",
      "    logger.setLevel(logging.INFO)\n",
      "    log_file_path = args.save_prefix + \u001b[33m'\u001b[39;49;00m\u001b[33m_train.log\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    log_dir = os.path.dirname(log_file_path)\n",
      "    \u001b[34mif\u001b[39;49;00m log_dir \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(log_dir):\n",
      "        os.makedirs(log_dir)\n",
      "    fh = logging.FileHandler(log_file_path)\n",
      "    logger.addHandler(fh)\n",
      "    logger.info(args)\n",
      "    logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mStart training from [Epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.start_epoch))\n",
      "    best_map = [\u001b[34m0\u001b[39;49;00m]\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(args.start_epoch, args.epochs):\n",
      "        \u001b[34mif\u001b[39;49;00m args.mixup:\n",
      "            \u001b[37m# TODO(zhreshold): more elegant way to control mixup during runtime\u001b[39;49;00m\n",
      "            \u001b[34mtry\u001b[39;49;00m:\n",
      "                train_data._dataset.set_mixup(np.random.beta, \u001b[34m1.5\u001b[39;49;00m, \u001b[34m1.5\u001b[39;49;00m)\n",
      "            \u001b[34mexcept\u001b[39;49;00m \u001b[36mAttributeError\u001b[39;49;00m:\n",
      "                train_data._dataset._data.set_mixup(np.random.beta, \u001b[34m1.5\u001b[39;49;00m, \u001b[34m1.5\u001b[39;49;00m)\n",
      "            \u001b[34mif\u001b[39;49;00m epoch >= args.epochs - args.no_mixup_epochs:\n",
      "                \u001b[34mtry\u001b[39;49;00m:\n",
      "                    train_data._dataset.set_mixup(\u001b[34mNone\u001b[39;49;00m)\n",
      "                \u001b[34mexcept\u001b[39;49;00m \u001b[36mAttributeError\u001b[39;49;00m:\n",
      "                    train_data._dataset._data.set_mixup(\u001b[34mNone\u001b[39;49;00m)\n",
      "\n",
      "        tic = time.time()\n",
      "        btic = time.time()\n",
      "        mx.nd.waitall()\n",
      "        net.hybridize()\n",
      "        \u001b[34mfor\u001b[39;49;00m i, batch \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(train_data):\n",
      "            batch_size = batch[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m0\u001b[39;49;00m]\n",
      "            data = gluon.utils.split_and_load(batch[\u001b[34m0\u001b[39;49;00m], ctx_list=ctx, batch_axis=\u001b[34m0\u001b[39;49;00m)\n",
      "            \u001b[37m# objectness, center_targets, scale_targets, weights, class_targets\u001b[39;49;00m\n",
      "            fixed_targets = [gluon.utils.split_and_load(batch[it], ctx_list=ctx, batch_axis=\u001b[34m0\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m it \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m1\u001b[39;49;00m, \u001b[34m6\u001b[39;49;00m)]\n",
      "            gt_boxes = gluon.utils.split_and_load(batch[\u001b[34m6\u001b[39;49;00m], ctx_list=ctx, batch_axis=\u001b[34m0\u001b[39;49;00m)\n",
      "            sum_losses = []\n",
      "            obj_losses = []\n",
      "            center_losses = []\n",
      "            scale_losses = []\n",
      "            cls_losses = []\n",
      "            \u001b[34mwith\u001b[39;49;00m autograd.record():\n",
      "                \u001b[34mfor\u001b[39;49;00m ix, x \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(data):\n",
      "                    obj_loss, center_loss, scale_loss, cls_loss = net(x, gt_boxes[ix], *[ft[ix] \u001b[34mfor\u001b[39;49;00m ft \u001b[35min\u001b[39;49;00m fixed_targets])\n",
      "                    sum_losses.append(obj_loss + center_loss + scale_loss + cls_loss)\n",
      "                    obj_losses.append(obj_loss)\n",
      "                    center_losses.append(center_loss)\n",
      "                    scale_losses.append(scale_loss)\n",
      "                    cls_losses.append(cls_loss)\n",
      "                autograd.backward(sum_losses)\n",
      "            trainer.step(batch_size)\n",
      "            obj_metrics.update(\u001b[34m0\u001b[39;49;00m, obj_losses)\n",
      "            center_metrics.update(\u001b[34m0\u001b[39;49;00m, center_losses)\n",
      "            scale_metrics.update(\u001b[34m0\u001b[39;49;00m, scale_losses)\n",
      "            cls_metrics.update(\u001b[34m0\u001b[39;49;00m, cls_losses)\n",
      "            \u001b[34mif\u001b[39;49;00m args.log_interval \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m (i + \u001b[34m1\u001b[39;49;00m) % args.log_interval:\n",
      "                name1, loss1 = obj_metrics.get()\n",
      "                name2, loss2 = center_metrics.get()\n",
      "                name3, loss3 = scale_metrics.get()\n",
      "                name4, loss4 = cls_metrics.get()\n",
      "                logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m][Batch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m], LR: \u001b[39;49;00m\u001b[33m{:.2E}\u001b[39;49;00m\u001b[33m, Speed: \u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m samples/sec, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "                    epoch, i, trainer.learning_rate, batch_size/(time.time()-btic), name1, loss1, name2, loss2, name3, loss3, name4, loss4))\n",
      "            btic = time.time()\n",
      "\n",
      "        name1, loss1 = obj_metrics.get()\n",
      "        name2, loss2 = center_metrics.get()\n",
      "        name3, loss3 = scale_metrics.get()\n",
      "        name4, loss4 = cls_metrics.get()\n",
      "\n",
      "        EVAL_METRICS[name1] = loss1\n",
      "        EVAL_METRICS[name2] = loss2\n",
      "        EVAL_METRICS[name3] = loss3\n",
      "        EVAL_METRICS[name4] = loss4\n",
      "\n",
      "        logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m] Training cost: \u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m{:.3f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\n",
      "            epoch, (time.time()-tic), name1, loss1, name2, loss2, name3, loss3, name4, loss4))\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m (epoch + \u001b[34m1\u001b[39;49;00m) % args.val_interval:\n",
      "            \u001b[37m# consider reduce the frequency of validation to save time\u001b[39;49;00m\n",
      "            map_name, mean_ap = validate(net, val_data, ctx, eval_metric, args.obj_metric)\n",
      "            EVAL_METRICS[\u001b[33m'\u001b[39;49;00m\u001b[33mmAP\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = mean_ap[\u001b[36mlen\u001b[39;49;00m(mean_ap)-\u001b[34m1\u001b[39;49;00m]\n",
      "\n",
      "            val_msg = \u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.join([\u001b[33m'\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(k, v) \u001b[34mfor\u001b[39;49;00m k, v \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(map_name, mean_ap)])\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m] Validation: \u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epoch, val_msg))\n",
      "            logger.info(\u001b[33m'\u001b[39;49;00m\u001b[33mTarget Objective: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{:.8f}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args.obj_metric, EVAL_METRICS[args.obj_metric]))\n",
      "            current_map = \u001b[36mfloat\u001b[39;49;00m(mean_ap[-\u001b[34m1\u001b[39;49;00m])\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            current_map = \u001b[34m0.\u001b[39;49;00m\n",
      "        save_params(net, best_map, current_map, epoch, args)\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    \n",
      "    args = parse_args()\n",
      "    \n",
      "    \u001b[37m# fix seed for mxnet, numpy and python builtin random generator.\u001b[39;49;00m\n",
      "    gutils.random.seed(args.seed)\n",
      "\n",
      "    \u001b[37m# training contexts\u001b[39;49;00m\n",
      "    \u001b[34mtry\u001b[39;49;00m:\n",
      "        a = mx.nd.zeros((\u001b[34m1\u001b[39;49;00m,), ctx=mx.gpu(\u001b[34m0\u001b[39;49;00m))\n",
      "        ctx = [mx.gpu(\u001b[36mint\u001b[39;49;00m(i)) \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m args.gpus.split(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mif\u001b[39;49;00m i.strip()]\n",
      "        \u001b[37m#print('GPU device is available')\u001b[39;49;00m\n",
      "    \u001b[34mexcept\u001b[39;49;00m:\n",
      "        ctx = [mx.cpu()]\n",
      "        \u001b[37m#print('Using CPU. GPU device was not detected')\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# load data\u001b[39;49;00m\n",
      "    train_dataset, val_dataset, classes = load_data()\n",
      "    \u001b[37m#print('copying classes.cv from {} to {}.'.format(CLASSES_FILE,os.path.join(MODEL_DIR, 'classes.csv')))\u001b[39;49;00m\n",
      "    \u001b[37m#shutil.copyfile(CLASSES_FILE, os.path.join(MODEL_DIR, 'classes.params'))\u001b[39;49;00m\n",
      "\n",
      "    \u001b[37m# network\u001b[39;49;00m\n",
      "    net_name = \u001b[33m'\u001b[39;49;00m\u001b[33m_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.join((\u001b[33m'\u001b[39;49;00m\u001b[33myolo3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, args.network, \u001b[33m'\u001b[39;49;00m\u001b[33mcustom\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    args.save_prefix += net_name\n",
      "    \n",
      "   \u001b[37m# MODEL_INFO = {\"base\":net_name, \"weights\":'{:s}_best.params'.format(net_name)}\u001b[39;49;00m\n",
      "   \u001b[37m# with open (os.path.join(MODEL_DIR,'model_info.params'),'w') as model_meta :\u001b[39;49;00m\n",
      "   \u001b[37m#     json.dump(MODEL_INFO,model_meta)\u001b[39;49;00m\n",
      "        \n",
      "    \n",
      "    \u001b[37m# use sync bn if specified\u001b[39;49;00m\n",
      "    TRANSFER = args.pre_trained_weights\n",
      "    PRETRAINED_BASE= \u001b[34mTrue\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m args.pre_trained_weights == \u001b[33m'\u001b[39;49;00m\u001b[33mNONE\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m :\n",
      "        TRANSFER = \u001b[34mNone\u001b[39;49;00m\n",
      "        PRETRAINED_BASE= \u001b[34mFalse\u001b[39;49;00m\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mNo pre-trained weights\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.syncbn \u001b[35mand\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(ctx) > \u001b[34m1\u001b[39;49;00m:\n",
      "        net = get_model(net_name, pretrained_base=PRETRAINED_BASE, \n",
      "                        norm_layer=gluon.contrib.nn.SyncBatchNorm,\n",
      "                        transfer = TRANSFER, classes= classes,\n",
      "                        norm_kwargs={\u001b[33m'\u001b[39;49;00m\u001b[33mnum_devices\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: \u001b[36mlen\u001b[39;49;00m(ctx)})\n",
      "        async_net = get_model(net_name, pretrained_base=\u001b[34mFalse\u001b[39;49;00m, classes= classes)  \u001b[37m# used by cpu worker\u001b[39;49;00m\n",
      "        \n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        net = get_model(net_name, pretrained_base=PRETRAINED_BASE, transfer = TRANSFER, classes = classes)\n",
      "        async_net = net\n",
      "        \n",
      "    \u001b[34mif\u001b[39;49;00m args.resume.strip():\n",
      "        MODEL_CHECKPOINT_WEIGHTS = args.resume.strip()\n",
      "        \u001b[36mprint\u001b[39;49;00m(MODEL_CHECKPOINT_WEIGHTS)\n",
      "        net.load_parameters(MODEL_CHECKPOINT_WEIGHTS)\n",
      "        async_net.load_parameters(MODEL_CHECKPOINT_WEIGHTS)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mwith\u001b[39;49;00m warnings.catch_warnings(record=\u001b[34mTrue\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m w:\n",
      "            warnings.simplefilter(\u001b[33m\"\u001b[39;49;00m\u001b[33malways\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            net.initialize()\n",
      "            async_net.initialize()\n",
      "\n",
      "    \u001b[37m# initialize loaders\u001b[39;49;00m\n",
      "    train_data, val_data = get_dataloader(\n",
      "        async_net, train_dataset, val_dataset, args.data_shape, args.batch_size, args.num_workers, args)\n",
      "\n",
      "    \u001b[37m# set metrics\u001b[39;49;00m\n",
      "    eval_metric = get_eval_metric(args, classes)\n",
      "\n",
      "    \u001b[37m# training\u001b[39;49;00m\n",
      "    train(net, train_data, val_data, eval_metric, ctx, args)\n",
      "    \n",
      "    \u001b[37m#print('attempting walk in model_dir..')\u001b[39;49;00m\n",
      "    \u001b[37m#for (dirpath, dirnames, filenames) in walk(MODEL_DIR):\u001b[39;49;00m\n",
      "    \u001b[37m#    for f in filenames :\u001b[39;49;00m\n",
      "    \u001b[37m#        print(f)\u001b[39;49;00m\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "!pygmentize ../scripts/gluoncv_yolo3-train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below runs the above script on this notebook. The script is only configure to run for one epoch because it can take over 5 minutes to run a single epoch on CPU.\n",
    "\n",
    "The YOLOv3 script has been configured to use mobilenet1.0 (versus darknet), and will be initialized with weights pretrained on the VOC dataset.\n",
    "\n",
    "It's **optional** to run this script locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory './models': File exists\n",
      "INFO:root:Namespace(batch_size=4, data_prefix='', data_shape=416, epochs=1, gpus='0', label_smooth=False, local=True, log_interval=100, lr=0.001, lr_decay=0.1, lr_decay_epoch='160,180', lr_decay_period=0, lr_mode='step', mixup=False, model_dir='./models', momentum=0.9, network='mobilenet1.0', no_mixup_epochs=20, no_random_shape=False, no_wd=False, num_samples=150, num_workers=1, obj_metric='mAP', opt='adam', pre_trained_weights='VOC', resume='', save_format='symbolic', save_interval=50, save_prefix='yolo3_mobilenet1.0_custom', seed=233, start_epoch=0, syncbn=True, test='../data/caltech_birds_sample/recordio/test', train='../data/caltech_birds_sample/recordio/train', val_interval=1, warmup_epochs=0, warmup_lr=0.0, wd=0.0005)\n",
      "INFO:root:Start training from [Epoch 0]\n",
      "INFO:root:[Epoch 0] Training cost: 10.102, ObjLoss=2491.172, BoxCenterLoss=2.382, BoxScaleLoss=1.335, ClassLoss=3.070\n",
      "INFO:root:[Epoch 0] Validation: \n",
      "017.Cardinal=0.0\n",
      "036.Northern_Flicker=0.0\n",
      "047.American_Goldfinch=0.0\n",
      "068.Ruby_throated_Hummingbird=0.0\n",
      "073.Blue_Jay=0.0\n",
      "mAP=0.0\n",
      "INFO:root:Target Objective: mAP = 0.00000000\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./models\n",
    "!python ../scripts/gluoncv_yolo3-train.py \\\n",
    "--model-dir ./models \\\n",
    "--train ../data/caltech_birds_sample/recordio/train \\\n",
    "--test ../data/caltech_birds_sample/recordio/test \\\n",
    "--lr 0.001 \\\n",
    "--opt adam \\\n",
    "--local True \\\n",
    "--pre-trained-weights VOC \\\n",
    "--num-workers 1 \\\n",
    "--save-format symbolic \\\n",
    "--network mobilenet1.0 --syncbn True --batch-size 4 --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate your Scripts\n",
    "\n",
    "You may eventually need to perform large-scale training that is beyond what want can be done from your local notebook. Before you leverage SageMaker traiing capabilities, you can run your training script in **\"local mode\"** to validate that your script will run within the SageMaker training environment.\n",
    "\n",
    "Local mode will deploy a SageMaker training container locally, and run your script. In the following steps we will setup our notebook to train in \"local mode.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies on SageMaker Managed Containers\n",
    "\n",
    "---\n",
    "\n",
    "One of the main benefits of leveraging the \"Bring-Your-Own-Script\" (BYOS) integration strategy is that it reduces alot of heavy-lifting compared to the alternative. For instance, if you want to integrate a custom algorithm in R with Amazon SageMaker, it is feasible. However, at this point in time, the BYOS integration option doesn't supported in R. Integrating your R algorithm would require you to build  an Amazon SageMaker compatible container to house your custom [algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html).\n",
    "\n",
    "MXNet, however, supports BYOS--Amazon SageMaker provides and maintains MXNet containers natively. However, in cases like the one in this lab, we may want to install or upgrade select depedencies. You have the option of creating your own SageMaker compatible container and use the MXNet container as a base, or update the dependencies at launch time using [requirements.txt](https://pip.pypa.io/en/stable/user_guide/#id15).\n",
    "\n",
    "DevOps best practices recommend building your own container. Requirements.txt is often ideal for development. You can apply minor updates quickly to support rapid prototyping.\n",
    "\n",
    "Since the dependencies vary based on GPU support, I've provided two variations of requirements.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Run the cell below to preview the dependency updates required by GluonCV for GPU enabled training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxnet-cu101==1.6.0.post0\n",
      "gluoncv==0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ../scripts/requirements-gpu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to preview the dependency updates required by GluonCV when your training instance doesn't have GPU available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mxnet-mkl==1.6.0\n",
      "gluoncv==0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ../scripts/requirements-cpu.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appropriate requirements.txt file has to be provided depending on which training instance you've selected. The requirements.txt needs to be place in the root directory of your scripts. In the following steps, we'll provide some utilities to take care of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Enabled Environment Considerations\n",
    "\n",
    "---\n",
    "Your environment and certain hyperparameters are dependent on the amount of available GPU. We need to ensure the right libraries are installed if you select a GPU enabled instance. \n",
    "\n",
    "Secondly, if you have multiple GPUs either on a single instance or distributed cluster, you have more GPU memory available to support larger batch sizes for accelerating training time. Larger batch sizes will help accelerate and scale your training process.\n",
    "\n",
    "This YOLOv3 script also needs to know how many devices you like to use for multiple GPU training on a single instance, so that data sharding can be done in the script to facilitate data parallelism scaling.\n",
    "\n",
    "Note that you may need to adapt to model performance degradation as a result of training on large batch sizes by lowering the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set requirements.txt for GPU:\n",
      "-----------------------------------\n",
      "mxnet-cu101==1.6.0.post0\n",
      "gluoncv==0.8.0\n",
      "-----------------------------------\n",
      "You have selected ml.g4dn.xlarge, which will utilize GPU.\n",
      "Recommended GPU dependent configurations: {'gpu_devices': '0', 'batch_size_range': [1, 16]}\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# sample selection of SageMaker instances and available GPUs\n",
    "INSTANCE_GPUS = {\n",
    "    'ml.p3.2xlarge':1,\n",
    "    'ml.p3.8xlarge':4,\n",
    "    'ml.c5.4xlarge':0,\n",
    "    'ml.c5.xlarge':0,\n",
    "    'ml.g4dn.xlarge':1\n",
    "}\n",
    "\n",
    "def set_requirements_text(gpu_train=True) :\n",
    "    \n",
    "    if gpu_train :\n",
    "        shutil.copy('../scripts/requirements-gpu.txt','../scripts/requirements.txt')\n",
    "    else :\n",
    "        shutil.copy('../scripts/requirements-cpu.txt','../scripts/requirements.txt')\n",
    "\n",
    "    !pygmentize ../scripts/requirements.txt\n",
    "\n",
    "def environment_device_config(instance_selection, acceleration_type=None) :\n",
    "    \n",
    "    USE_GPU = (INSTANCE_GPUS[instance_selection] > 0)\n",
    "\n",
    "    gpu_devices = '0'\n",
    "    for i in range(1,INSTANCE_GPUS[instance_selection]) :\n",
    "        gpu_devices+= \",\"+str(i)\n",
    "\n",
    "    print('Set requirements.txt for {}:'.format('GPU' if USE_GPU else 'CPU')) \n",
    "    print('-----------------------------------')\n",
    "    set_requirements_text(USE_GPU)\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    config={\n",
    "        \"gpu_devices\":gpu_devices,\n",
    "        \"batch_size_range\": [1*INSTANCE_GPUS[instance_selection] if USE_GPU else 16,\n",
    "                          16*INSTANCE_GPUS[instance_selection] if USE_GPU else 32]\n",
    "    }\n",
    "    \n",
    "    print('You have selected {}{}, which will utilize {}.'.format(instance_selection, \n",
    "                                                                '' if acceleration_type == None else \" with EIA: {}\".format(acceleration_type),\n",
    "                                                                'GPU' if USE_GPU else 'CPU' if acceleration_type == None else \"EIA\"))\n",
    "    print('Recommended GPU dependent configurations: {}'.format(config))\n",
    "    \n",
    "    return config\n",
    "\n",
    "LOCAL_INSTANCE_TYPE = 'ml.g4dn.xlarge'\n",
    "config = environment_device_config(LOCAL_INSTANCE_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Amazon SageMaker Local\n",
    "\n",
    "We're now ready to configure our script with appropriate hyperparameters, and launch Amazon SageMaker training in local mode. \n",
    "\n",
    "We'll use the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'optimizer': 'adam', 'lr': 0.0001, 'epochs': 1, 'batch-size': 8.0, 'network': 'mobilenet1.0', 'syncbn': True, 'num-workers': 1, 'save-format': 'symbolic', 'pre-trained-weights': 'VOC'}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.0001,\n",
    "    'epochs': 1,\n",
    "    'batch-size': np.floor((config['batch_size_range'][0]+config['batch_size_range'][1])/2),\n",
    "    'network':'mobilenet1.0',\n",
    "    'syncbn':True,\n",
    "    'num-workers':1,\n",
    "    'save-format':'symbolic',\n",
    "    'pre-trained-weights':\"VOC\"} \n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample dataset of the Caltech Birds dataset in RecordIO format should have been copied over to your notebook through the Github integration. Run the cell to validate the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local training data directory contents:\n",
      "---------------------------------------\n",
      "classes.csv\n",
      "train.rec\n",
      "train.idx\n",
      "\n",
      "Local test data directory contents:\n",
      "---------------------------------------\n",
      "val.idx\n",
      "val.rec\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "LOCAL_TRAIN_DATA_DIR = os.path.join(os.getcwd(),'../data/caltech_birds_sample/recordio/train')\n",
    "LOCAL_TEST_DATA_DIR = os.path.join(os.getcwd(),'../data/caltech_birds_sample/recordio/test')\n",
    "\n",
    "print('Local training data directory contents:')\n",
    "print('---------------------------------------')\n",
    "for f in listdir(LOCAL_TRAIN_DATA_DIR) :\n",
    "    print(f)\n",
    "print()\n",
    "print('Local test data directory contents:')\n",
    "print('---------------------------------------')\n",
    "for f in listdir(LOCAL_TEST_DATA_DIR) :\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate an MXNet estimator. This object ecapsulates your MXNet algorithm including infrastructure configurations that it will leverage when you execute training via the fit() method.\n",
    "\n",
    "The parameter **train_instance_type = 'local'** informs SageMaker that you wish to run this estimator in local mode. Secondly, ensure you replace /'<<NAME YOUR TRAINING JOB/>>' with a unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install 'sagemaker[local]' --upgrade\n",
    "\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "local_session = LocalSession()\n",
    "local_session.config = {'local': {'local_code': True}}\n",
    "\n",
    "BASE_JOB_NAME ='dylantong-yolo3-local-training'\n",
    "#BASE_JOB_NAME='<<NAME YOUR TRAINING JOB>>'\n",
    "SOURCE_DIR = '../scripts'\n",
    "\n",
    "local_estimator = MXNet(entry_point='gluoncv_yolo3-train.py',\n",
    "                        role=role,\n",
    "                        sagemaker_session = local_session,\n",
    "                        source_dir=SOURCE_DIR,\n",
    "                        framework_version=FRAMEWORK_VERSION,\n",
    "                        py_version='py3',\n",
    "                        train_instance_count=1,\n",
    "                        train_instance_type='local',\n",
    "                        hyperparameters=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fit() will deploy a training container locally to run your training script. In local mode, you have the option of utilizing data from a local file system as shown below. Alternatively, you can specify S3 locations as would normally do for remote training (commented out below).\n",
    "\n",
    "**SageMaker local mode doesn't work with in SageMaker Studio at this time 8/27/2020. Please skip this cell for now.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "'docker-compose' is not installed. Local Mode features will not work without docker-compose. For more information on how to install 'docker-compose', please, see https://docs.docker.com/compose/install/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4442ac526163>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLOCAL_TEST_INPUT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL_TEST_DATA_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlocal_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLOCAL_TRAIN_INPUT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLOCAL_TEST_INPUT\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mtrain_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"enable_sagemaker_metrics\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_sagemaker_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image, algorithm_arn, encrypt_inter_container_traffic, train_use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     def process(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mResourceConfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"InstanceCount\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mAlgorithmSpecification\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"TrainingImage\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m     99\u001b[0m         \u001b[0mtraining_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalTrainingJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, instance_type, instance_count, image, sagemaker_session)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfind_executable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docker-compose\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             raise ImportError(\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;34m\"'docker-compose' is not installed. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;34m\"Local Mode features will not work without docker-compose. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;34m\"For more information on how to install 'docker-compose', please, see \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: 'docker-compose' is not installed. Local Mode features will not work without docker-compose. For more information on how to install 'docker-compose', please, see https://docs.docker.com/compose/install/"
     ]
    }
   ],
   "source": [
    "LOCAL_TRAIN_INPUT = local_session.upload_data(path=LOCAL_TRAIN_DATA_DIR)\n",
    "LOCAL_TEST_INPUT = local_session.upload_data(path=LOCAL_TEST_DATA_DIR)\n",
    "\n",
    "local_estimator.fit({\"train\":LOCAL_TRAIN_INPUT, \"test\":LOCAL_TEST_INPUT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Train the model\n",
    "\n",
    "So you've identified candidate algorithm(s) and an architecture that is ready for larger scale training, and you've validated your script in a SageMaker training environment via SageMaker Local. \n",
    "\n",
    "You can now leverage Amazon SageMaker to perform large-scale training with zero setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define your hyperparameters\n",
    "\n",
    "After you've verified that your script runs on Amazon SageMaker training without errors, a good first step is utilize the SageMaker's Automatic Model Tuning to hone in a good set of hyperparameters.\n",
    "\n",
    "Let's prepare configurations for GPU training so our tuning job can get done in a reasonable time. With SageMaker Training you only pay for the resources during training. GPU instances are more expensive, but they can also reduce training time by magnitudes compared to training on CPU, which in turn reduces training time costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set requirements.txt for GPU:\n",
      "-----------------------------------\n",
      "mxnet-cu101==1.6.0.post0\n",
      "gluoncv==0.8.0\n",
      "-----------------------------------\n",
      "You have selected ml.p3.8xlarge, which will utilize GPU.\n",
      "Recommended GPU dependent configurations: {'gpu_devices': '0,1,2,3', 'batch_size_range': [4, 64]}\n"
     ]
    }
   ],
   "source": [
    "INSTANCE_SELECTION = 'ml.p3.8xlarge'\n",
    "config = environment_device_config(INSTANCE_SELECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to specify the objective metric so that SageMaker can use Bayesian Optimization to find hyperparameters that optimize on a specific objective. We'll use the standard objective, which is to maximize mAP on the validation set.\n",
    "\n",
    "The regex experession informs SageMaker how to parse the training logs to obtain the configured objective metric. Your script should have logic to write out the objective metric into the logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Name': 'mAP', 'Regex': 'Target Objective: mAP = ([0-9\\\\.]+)'}]\n"
     ]
    }
   ],
   "source": [
    "OBJECTIVE_METRIC_NAME = 'mAP'\n",
    "OBJECTIVE_TYPE = 'Maximize'\n",
    "\n",
    "METRIC_DEFINITIONS = [{ 'Name':OBJECTIVE_METRIC_NAME,\n",
    "                        'Regex':'Target Objective: {} = ([0-9\\\\.]+)'.format(OBJECTIVE_METRIC_NAME)}]\n",
    "\n",
    "print(METRIC_DEFINITIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the hyperparameters as well as the the hyperparameter value ranges that you want Amazon SageMaker to search over.\n",
    "\n",
    "The configurations below will search for optimal values for the optimizer to use, the learning rate, and the mini-batch-size. In practice, you might want to explore other variables such as the base network, pretrained weights, and other variables that could influence the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'opt': 'adam', 'epochs': 1, 'network': 'mobilenet1.0', 'syncbn': True, 'gpus': '0,1,2,3'}\n"
     ]
    }
   ],
   "source": [
    "HYPERPARAMETER_RANGES = {'opt': CategoricalParameter(['adam','sgd']),\n",
    "                         'lr': ContinuousParameter(0.0002, 0.001, scaling_type='Logarithmic'),\n",
    "                         'batch_size': IntegerParameter(config['batch_size_range'][0], \n",
    "                                                        config['batch_size_range'][1])}\n",
    "params = {\n",
    "    'opt': 'adam',\n",
    "    'epochs': 1,\n",
    "    'network':'mobilenet1.0',\n",
    "    'syncbn':True,\n",
    "    'gpus': config['gpu_devices'] \n",
    "} \n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Launch your Hyperparameter Tuning Job\n",
    "\n",
    "Similar to Lab 2, we're going to launch an hyperparamter job to automate the hyperparameter search process. The difference is that we're launching this job programatically via the Python SDK. \n",
    "\n",
    "Secondly, since we're training on a custom script, we have a couple of additional parameters: (1) we specify the location of the scripts in our local directory (source_dir) (2) we provide the name of our MXNet (GluonCV) script that serves as an entry point (entry_point) for our algorithm. \n",
    "\n",
    "Lastly, we have provided a requirements.txt file in the same directory as our scripts, so that it will be loaded into our training containers. Requirements.txt will instruct SageMaker to install the necessary dependencies on to the training containers upon launch. The values framework_version='1.3.0' and py_version='py3' informs SageMaker to use the MXNet container that has been pre-built and tested for MXNet1.3 running on python3.\n",
    "\n",
    "Replace '\\<<NAME YOUR HPO JOB\\>>' with a unique name, so that you can identify your hyperparameter tuning jobs.\n",
    "\n",
    "In this cell we instantiate an MXNet estimator. This object ecapsulates your MXNet algorithm including infrastructure configurations that it will leverage when you execute training via the fit() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_JOB_NAME='dylantong-yolo3-hpo'\n",
    "#BASE_JOB_NAME='<<NAME YOUR HPO JOB>>'\n",
    "SOURCE_DIR = '../scripts'\n",
    "\n",
    "estimator = MXNet(entry_point='gluoncv_yolo3-train.py',\n",
    "                  role=role,\n",
    "                  source_dir=SOURCE_DIR,\n",
    "                  framework_version=FRAMEWORK_VERSION,\n",
    "                  py_version='py3',\n",
    "                  train_instance_count=1,\n",
    "                  train_instance_type=INSTANCE_SELECTION,\n",
    "                  sagemaker_session=sagemaker.Session(),\n",
    "                  base_job_name=BASE_JOB_NAME,\n",
    "                  hyperparameters=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the parameters for our automatic hyperparameter tuner job. We provide the same parameters as we did in Lab 2 except we're instantiating the job programmatically via the SDK. Note that we only run two jobs (max_jobs=2) for the sake of demonstration. In practice, you'll need to run more to hone in on optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            OBJECTIVE_METRIC_NAME,\n",
    "                            HYPERPARAMETER_RANGES,\n",
    "                            METRIC_DEFINITIONS,\n",
    "                            objective_type = OBJECTIVE_TYPE,\n",
    "                            max_jobs=1,\n",
    "                            max_parallel_jobs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling fit() on our tuner launch the job as a non-blocking asynchronous call. We pass two parameters to specify the location of our training and test sets.\n",
    "\n",
    "'Train' and 'Test' will be treated as \"channels\" and the training container will have environment variables with the name SM_CHANNEL_TRAIN and SM_CHANNEL_TEST created respectively. The datasets will be loaded from the specified S3 location, and can be access in the training container context via the environment variables (eg. os.environ['SM_CHANNEL_TRAIN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': SAMPLE_TRAINING_DATASET, \n",
    "           'test': SAMPLE_TEST_DATASET})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker's Automatic Model tuning functionality can run tuning jobs in sequence and in parallel asynchronously. You can use the .wait() command to block a thread and wait for completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Finetune your model\n",
    "\n",
    "The training process will be iterative as described in Lab 2. Eventually, you will hone in on a set of parameters, and may choose to further fine tune the model.\n",
    "\n",
    "The section below demonstrates how to perform incremental training on a model that you've previously trained. For instance, this could be the best model from your recent Hyperparameter Tuning Job. \n",
    "\n",
    "The training time required to train a YOLOv3 model is beyond what is practical for a workshop. Therefore, I've provided a pretrained YOLOv3 model that has been finetuned a sample of our CalTech Birds dataset. You can treat this model as if it were the output of your Hyperparameter Tuning Job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL = 'yolo3_mobilenet1.0_custom_best.params'\n",
    "TRAINED_MODEL_URI = 's3://reinvent2018-sagemaker-pytorch/models/caltech-birds/gluoncv/yolov3/sample/{}'.format(TRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select an instance and rest our parameters. We'll use a smaller GPU instance for the purpose of minimizing the cost of this lab.\n",
    "\n",
    "Note that we've also set the epochs to one for the sake of demonstration. The provided model has been trained for hundreds of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set requirements.txt for GPU:\n",
      "-----------------------------------\n",
      "mxnet-cu101==1.6.0.post0\n",
      "gluoncv==0.8.0\n",
      "-----------------------------------\n",
      "You have selected ml.p3.2xlarge, which will utilize GPU.\n",
      "Recommended GPU dependent configurations: {'gpu_devices': '0', 'batch_size_range': [1, 16]}\n",
      "\n",
      "Configured Parameters\n",
      "---------------------\n",
      "{'optimizer': 'adam', 'lr': 0.0001, 'epochs': 1, 'batch-size': 16, 'network': 'mobilenet1.0', 'syncbn': True, 'pre-trained-weights': 'NONE', 'resume': 'yolo3_mobilenet1.0_custom_best.params', 'save-format': 'symbolic', 'gpus': '0'}\n"
     ]
    }
   ],
   "source": [
    "INSTANCE_SELECTION = 'ml.p3.2xlarge'\n",
    "config = environment_device_config(INSTANCE_SELECTION)\n",
    "\n",
    "params = {\n",
    "    'optimizer': 'adam',\n",
    "    'lr': 0.0001,\n",
    "    'epochs': 1,\n",
    "    'batch-size': config['batch_size_range'][1],\n",
    "    'network':'mobilenet1.0',\n",
    "    'syncbn':True,\n",
    "    'pre-trained-weights':\"NONE\",\n",
    "    'resume': TRAINED_MODEL,\n",
    "    'save-format': 'symbolic',\n",
    "    'gpus': config['gpu_devices'] } \n",
    "print()\n",
    "print(\"Configured Parameters\")\n",
    "print(\"---------------------\")\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace '\\<<NAME YOUR TRAINING JOB\\>>' with an unique prefix that will help you identify your training job.\n",
    "\n",
    "We're going to instantiate another MXNet estimator object. The main difference is the \"model_uri\" parameter, which will result in having the model artifacts at the specified S3 location to be uploaded into the training container. Your script can then initialize a model with those weights. The artifacts are located in the directory specified by the SM_CHANNEL_MODEL environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_JOB_NAME ='dylantong-yolo3-incremental-training'\n",
    "#BASE_JOB_NAME='<<NAME YOUR TRAINING JOB>>'\n",
    "SOURCE_DIR = '../scripts'\n",
    "\n",
    "incr_estimator = MXNet(entry_point='gluoncv_yolo3-train.py',\n",
    "                       role=role,\n",
    "                       source_dir=SOURCE_DIR,\n",
    "                       framework_version=FRAMEWORK_VERSION,\n",
    "                       py_version='py3',\n",
    "                       train_instance_count=1,\n",
    "                       train_instance_type=INSTANCE_SELECTION,\n",
    "                       sagemaker_session=sagemaker.Session(),\n",
    "                       base_job_name=BASE_JOB_NAME,\n",
    "                       hyperparameters=params,\n",
    "                       model_uri=TRAINED_MODEL_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-28 07:45:08 Starting - Starting the training job...\n",
      "2020-08-28 07:45:10 Starting - Launching requested ML instances......\n",
      "2020-08-28 07:46:18 Starting - Preparing the instances for training......\n",
      "2020-08-28 07:47:27 Downloading - Downloading input data...\n",
      "2020-08-28 07:47:53 Training - Downloading the training image.....\u001b[34m2020-08-28 07:48:49,288 sagemaker-training-toolkit INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-08-28 07:48:49,312 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch-size\":16,\"epochs\":1,\"gpus\":\"0\",\"lr\":0.0001,\"network\":\"mobilenet1.0\",\"optimizer\":\"adam\",\"pre-trained-weights\":\"NONE\",\"resume\":\"yolo3_mobilenet1.0_custom_best.params\",\"save-format\":\"symbolic\",\"syncbn\":true}', 'SM_USER_ENTRY_POINT': 'gluoncv_yolo3-train.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"model\",\"test\",\"train\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'gluoncv_yolo3-train', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '8', 'SM_NUM_GPUS': '1', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-west-2-407247006381/dylantong-yolo3-incremental-training-2020-08-28-07-45-08-264/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"epochs\":1,\"gpus\":\"0\",\"lr\":0.0001,\"network\":\"mobilenet1.0\",\"optimizer\":\"adam\",\"pre-trained-weights\":\"NONE\",\"resume\":\"yolo3_mobilenet1.0_custom_best.params\",\"save-format\":\"symbolic\",\"syncbn\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"dylantong-yolo3-incremental-training-2020-08-28-07-45-08-264\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-407247006381/dylantong-yolo3-incremental-training-2020-08-28-07-45-08-264/source/sourcedir.tar.gz\",\"module_name\":\"gluoncv_yolo3-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gluoncv_yolo3-train.py\"}', 'SM_USER_ARGS': '[\"--batch-size\",\"16\",\"--epochs\",\"1\",\"--gpus\",\"0\",\"--lr\",\"0.0001\",\"--network\",\"mobilenet1.0\",\"--optimizer\",\"adam\",\"--pre-trained-weights\",\"NONE\",\"--resume\",\"yolo3_mobilenet1.0_custom_best.params\",\"--save-format\",\"symbolic\",\"--syncbn\",\"True\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TEST': '/opt/ml/input/data/test', 'SM_CHANNEL_MODEL': '/opt/ml/input/data/model', 'SM_CHANNEL_TRAIN': '/opt/ml/input/data/train', 'SM_HP_RESUME': 'yolo3_mobilenet1.0_custom_best.params', 'SM_HP_LR': '0.0001', 'SM_HP_PRE-TRAINED-WEIGHTS': 'NONE', 'SM_HP_SAVE-FORMAT': 'symbolic', 'SM_HP_NETWORK': 'mobilenet1.0', 'SM_HP_BATCH-SIZE': '16', 'SM_HP_OPTIMIZER': 'adam', 'SM_HP_GPUS': '0', 'SM_HP_EPOCHS': '1', 'SM_HP_SYNCBN': 'true'}\u001b[0m\n",
      "\u001b[34m2020-08-28 07:48:49,616 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting mxnet-cu101==1.6.0.post0\n",
      "  Downloading https://files.pythonhosted.org/packages/bf/03/02325a5de95d5cfdd43c929ea55d9cadb44d239ca3aee7e3131540c09773/mxnet_cu101-1.6.0.post0-py2.py3-none-manylinux1_x86_64.whl (711.7MB)\u001b[0m\n",
      "\n",
      "2020-08-28 07:48:48 Training - Training image download completed. Training in progress.\u001b[34mCollecting gluoncv==0.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/d7/74b530c461ac3eb90f6045a645a59450de1f3d616a4926e371daa021dbd8/gluoncv-0.8.0-py2.py3-none-any.whl (810kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/site-packages (from mxnet-cu101==1.6.0.post0->-r requirements.txt (line 1)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/site-packages (from mxnet-cu101==1.6.0.post0->-r requirements.txt (line 1)) (0.8.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.6/site-packages (from mxnet-cu101==1.6.0.post0->-r requirements.txt (line 1)) (2.22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/site-packages (from gluoncv==0.8.0->-r requirements.txt (line 2)) (3.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: portalocker in /usr/local/lib/python3.6/site-packages (from gluoncv==0.8.0->-r requirements.txt (line 2)) (2.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /usr/local/lib/python3.6/site-packages (from gluoncv==0.8.0->-r requirements.txt (line 2)) (7.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /usr/local/lib/python3.6/site-packages (from gluoncv==0.8.0->-r requirements.txt (line 2)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /usr/local/lib/python3.6/site-packages (from gluoncv==0.8.0->-r requirements.txt (line 2)) (4.39.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101==1.6.0.post0->-r requirements.txt (line 1)) (2020.6.20)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101==1.6.0.post0->-r requirements.txt (line 1)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101==1.6.0.post0->-r requirements.txt (line 1)) (1.25.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/site-packages (from requests<3,>=2.20.0->mxnet-cu101==1.6.0.post0->-r requirements.txt (line 1)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/site-packages (from matplotlib->gluoncv==0.8.0->-r requirements.txt (line 2)) (0.10.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/site-packages (from matplotlib->gluoncv==0.8.0->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/site-packages (from matplotlib->gluoncv==0.8.0->-r requirements.txt (line 2)) (1.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib->gluoncv==0.8.0->-r requirements.txt (line 2)) (2.8.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->gluoncv==0.8.0->-r requirements.txt (line 2)) (1.15.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: mxnet-cu101, gluoncv\u001b[0m\n",
      "\u001b[34m  Found existing installation: gluoncv 0.6.0\n",
      "    Uninstalling gluoncv-0.6.0:\n",
      "      Successfully uninstalled gluoncv-0.6.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed gluoncv-0.8.0 mxnet-cu101-1.6.0.post0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 19.3.1; however, version 20.2.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-08-28 07:49:23,131 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"model\": \"/opt/ml/input/data/model\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"resume\": \"yolo3_mobilenet1.0_custom_best.params\",\n",
      "        \"lr\": 0.0001,\n",
      "        \"pre-trained-weights\": \"NONE\",\n",
      "        \"save-format\": \"symbolic\",\n",
      "        \"network\": \"mobilenet1.0\",\n",
      "        \"batch-size\": 16,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"gpus\": \"0\",\n",
      "        \"epochs\": 1,\n",
      "        \"syncbn\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"model\": {\n",
      "            \"ContentType\": \"application/x-sagemaker-model\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"dylantong-yolo3-incremental-training-2020-08-28-07-45-08-264\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-407247006381/dylantong-yolo3-incremental-training-2020-08-28-07-45-08-264/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"gluoncv_yolo3-train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"gluoncv_yolo3-train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":16,\"epochs\":1,\"gpus\":\"0\",\"lr\":0.0001,\"network\":\"mobilenet1.0\",\"optimizer\":\"adam\",\"pre-trained-weights\":\"NONE\",\"resume\":\"yolo3_mobilenet1.0_custom_best.params\",\"save-format\":\"symbolic\",\"syncbn\":true}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=gluoncv_yolo3-train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"model\",\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=gluoncv_yolo3-train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-407247006381/dylantong-yolo3-incremental-training-2020-08-28-07-45-08-264/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"model\":\"/opt/ml/input/data/model\",\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":16,\"epochs\":1,\"gpus\":\"0\",\"lr\":0.0001,\"network\":\"mobilenet1.0\",\"optimizer\":\"adam\",\"pre-trained-weights\":\"NONE\",\"resume\":\"yolo3_mobilenet1.0_custom_best.params\",\"save-format\":\"symbolic\",\"syncbn\":true},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"model\":{\"ContentType\":\"application/x-sagemaker-model\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"dylantong-yolo3-incremental-training-2020-08-28-07-45-08-264\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-407247006381/dylantong-yolo3-incremental-training-2020-08-28-07-45-08-264/source/sourcedir.tar.gz\",\"module_name\":\"gluoncv_yolo3-train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"gluoncv_yolo3-train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"16\",\"--epochs\",\"1\",\"--gpus\",\"0\",\"--lr\",\"0.0001\",\"--network\",\"mobilenet1.0\",\"--optimizer\",\"adam\",\"--pre-trained-weights\",\"NONE\",\"--resume\",\"yolo3_mobilenet1.0_custom_best.params\",\"--save-format\",\"symbolic\",\"--syncbn\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_MODEL=/opt/ml/input/data/model\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_RESUME=yolo3_mobilenet1.0_custom_best.params\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_PRE-TRAINED-WEIGHTS=NONE\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE-FORMAT=symbolic\u001b[0m\n",
      "\u001b[34mSM_HP_NETWORK=mobilenet1.0\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_SYNCBN=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 gluoncv_yolo3-train.py --batch-size 16 --epochs 1 --gpus 0 --lr 0.0001 --network mobilenet1.0 --optimizer adam --pre-trained-weights NONE --resume yolo3_mobilenet1.0_custom_best.params --save-format symbolic --syncbn True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mNo pre-trained weights\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/model/yolo3_mobilenet1.0_custom_best.params\u001b[0m\n",
      "\u001b[34mINFO:root:Namespace(batch_size=16, data_prefix='', data_shape=416, epochs=1, gpus='0', label_smooth=False, local=False, log_interval=100, lr=0.0001, lr_decay=0.1, lr_decay_epoch='160,180', lr_decay_period=0, lr_mode='step', mixup=False, model_dir='/opt/ml/model', momentum=0.9, network='mobilenet1.0', no_mixup_epochs=20, no_random_shape=False, no_wd=False, num_samples=150, num_workers=4, obj_metric='mAP', opt='adam', pre_trained_weights='NONE', resume='/opt/ml/input/data/model/yolo3_mobilenet1.0_custom_best.params', save_format='symbolic', save_interval=50, save_prefix='yolo3_mobilenet1.0_custom', seed=233, start_epoch=0, syncbn=True, test='/opt/ml/input/data/test', train='/opt/ml/input/data/train', val_interval=1, warmup_epochs=0, warmup_lr=0.0, wd=0.0005)\u001b[0m\n",
      "\u001b[34mINFO:root:Start training from [Epoch 0]\u001b[0m\n",
      "\u001b[34mINFO:root:[Epoch 0] Training cost: 6.851, ObjLoss=1.018, BoxCenterLoss=1.815, BoxScaleLoss=0.352, ClassLoss=0.143\u001b[0m\n",
      "\u001b[34mINFO:root:[Epoch 0] Validation: \u001b[0m\n",
      "\u001b[34m017.Cardinal=0.7701767676767677\u001b[0m\n",
      "\u001b[34m036.Northern_Flicker=0.7474747474747473\u001b[0m\n",
      "\u001b[34m047.American_Goldfinch=0.8747714211076281\u001b[0m\n",
      "\u001b[34m068.Ruby_throated_Hummingbird=0.8111888111888114\u001b[0m\n",
      "\u001b[34m073.Blue_Jay=0.7843142729506367\u001b[0m\n",
      "\u001b[34mmAP=0.7975852040797182\u001b[0m\n",
      "\u001b[34mINFO:root:Target Objective: mAP = 0.79758520\u001b[0m\n",
      "\u001b[34m2020-08-28 07:49:50,084 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-08-28 07:49:57 Uploading - Uploading generated training model\n",
      "2020-08-28 07:50:30 Completed - Training job completed\n",
      "Training seconds: 183\n",
      "Billable seconds: 183\n"
     ]
    }
   ],
   "source": [
    "incr_estimator.fit({'train': SAMPLE_TRAINING_DATASET, \n",
    "                    'test': SAMPLE_TEST_DATASET})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to programatically deploy our model, so that we have a managed endpoint for inference. \n",
    "\n",
    "This requires us to provide a script that is capable of loading a model into a model serving container and responding to requests. SageMaker expects integrators to implement a programmatic [interface](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#model-serving). Refer to the [link](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#model-serving) for details. The interface is common among all the BYOS integrations (eg. Tensorflow, PyTorch, MXNet).\n",
    "\n",
    "The interface has default implementations, but generally, you will need to provide your own implementation. Run the cell below to view the model serving script that has been provided for the YOLO3 model. The model expects images in a binary format, and returns three tensors (class, scores, bounding box) as a byte stream in a JSON response. \n",
    "\n",
    "You have to freedom to accept and respond with different formats, and there are certainly performance improvements that can be made with the current implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m walk\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmx\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gluon\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgluoncv\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mgcv\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_sym_model\u001b[39;49;00m(sym_f, param_f, model_dir) :\n",
      "\n",
      "\u001b[37m#    try:\u001b[39;49;00m\n",
      "\u001b[37m#        a = mx.nd.zeros((1,), ctx=mx.gpu(0))\u001b[39;49;00m\n",
      "\u001b[37m#        device = [mx.gpu(0)]\u001b[39;49;00m\n",
      "\u001b[37m#        print('GPU device is available')\u001b[39;49;00m\n",
      "\u001b[37m#    except:\u001b[39;49;00m\n",
      "    device = [mx.cpu()]\n",
      "\u001b[37m#        print('Using CPU on local machine. GPU device was not detected')\u001b[39;49;00m\n",
      "\n",
      "    sym_file = os.path.join(model_dir, sym_f)\n",
      "    param_file = os.path.join(model_dir, param_f)\n",
      "    \u001b[34mreturn\u001b[39;49;00m gluon.nn.SymbolBlock.imports(sym_file, [\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], param_file, ctx=device)\n",
      "       \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \n",
      "    model = load_sym_model(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSYM_FILE_NAME\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mPARAM_FILE_NAME\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], model_dir)\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(request_body, request_content_type):\n",
      "    \n",
      "    input_object, image = gcv.data.transforms.presets.yolo.transform_test(mx.img.imdecode(request_body), \u001b[34m512\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m input_object\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_object, model):\n",
      "\n",
      "    cid, score, bbox = model(input_object)  \n",
      "    \n",
      "    c= cid[\u001b[34m0\u001b[39;49;00m].asnumpy().reshape(cid[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m0\u001b[39;49;00m]*cid[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m1\u001b[39;49;00m])\n",
      "    s=score[\u001b[34m0\u001b[39;49;00m].asnumpy().reshape(score[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m0\u001b[39;49;00m]*score[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m1\u001b[39;49;00m])\n",
      "    bb= bbox[\u001b[34m0\u001b[39;49;00m].asnumpy().reshape(bbox[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m0\u001b[39;49;00m]*bbox[\u001b[34m0\u001b[39;49;00m].shape[\u001b[34m1\u001b[39;49;00m])\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m np.concatenate((c,s,bb))\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction, content_type):\n",
      "    \n",
      "    response_body = prediction.tobytes()\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m response_body\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ../scripts/gluoncv-yolo3-inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by selecting our instance, and ensuring the right dependencies will be installed on our model serving instance. We'll use a c5, a CPU optimize instance, but attach GPU to the instance via Elastic Inference Acceleration (EIA). GPU should provide us better throughput on YOLOv3. With EIA, you'll get performance that betters the K80 GPUs that are available on the P2 instances at a lower cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set requirements.txt for CPU:\n",
      "-----------------------------------\n",
      "mxnet-mkl==1.6.0\n",
      "gluoncv==0.8.0\n",
      "-----------------------------------\n",
      "You have selected ml.c5.xlarge with EIA: ml.eia1.medium, which will utilize EIA.\n",
      "Recommended GPU dependent configurations: {'gpu_devices': '0', 'batch_size_range': [16, 32]}\n"
     ]
    }
   ],
   "source": [
    "INSTANCE_SELECTION = 'ml.c5.xlarge'\n",
    "ACCELERATION_TYPE = 'ml.eia1.medium'\n",
    "#ACCELERATION_TYPE = None\n",
    "USE_EIA = ACCELERATION_TYPE is not None\n",
    "\n",
    "config = environment_device_config(INSTANCE_SELECTION, ACCELERATION_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's programatically register the last model that we trained, so that it is available for deployment within SageMaker Hosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8/28/2020 MXNET 1.6 containers don't have support for elastic inference. Using MXNet 1.5.1 for serving temporarily.\n",
    "INFERENCE_MXNET_VER = '1.5.1'\n",
    "env_vars = {\"SAGEMAKER_MODEL_SERVER_TIMEOUT\":\"3600\",\n",
    "            \"SYM_FILE_NAME\":\"yolov3-gcv-symbol.json\",\n",
    "            \"PARAM_FILE_NAME\":\"yolov3-gcv-0000.params\",\n",
    "            \"USE_EIA\": str(int(USE_EIA))}\n",
    "                              \n",
    "\n",
    "yolo_model = MXNetModel(model_data = incr_estimator.model_data,\n",
    "                        role=role,\n",
    "                        framework_version=INFERENCE_MXNET_VER,\n",
    "                        py_version=\"py3\",\n",
    "                        entry_point='gluoncv-yolo3-inference.py',\n",
    "                        env = env_vars,\n",
    "                        source_dir=incr_estimator.source_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace \\'<<PROVIDE AN ENDPOINT NAME\\>>' with an unique name so that you can identify your endpoint. For the purpose of demonstration, we only deploy a single instance. However, the endpoint can be configured for auto-scaling and deployed across a fleet to support throughput requirements.\n",
    "\n",
    "**This may take around 10 minutes to deploy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name: dylantong-yolov3-rt-ep-2020-08-28-20-06-30\n",
      "-------------!Endpoint was deployed in 411.327956 seconds.\n"
     ]
    }
   ],
   "source": [
    "RT_ENDPOINT_NAME = 'dylantong-yolov3-rt-ep-{}'.format(datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "#RT_ENDPOINT_NAME = '<<PROVIDE AN ENDPOINT NAME>>'\n",
    "print('endpoint name: {}'.format(RT_ENDPOINT_NAME))\n",
    "\n",
    "start = time.time()\n",
    "yolo_rt_ep = yolo_model.deploy(instance_type=INSTANCE_SELECTION,\n",
    "                                initial_instance_count = 1,\n",
    "                                endpoint_name= RT_ENDPOINT_NAME,\n",
    "                                accelerator_type=ACCELERATION_TYPE)\n",
    "end = time.time()\n",
    "\n",
    "print(\"Endpoint was deployed in {:3f} seconds.\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download some sample images from the internet, and see how well our bird detector does on detecting these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O multi-goldfinch-1.jpg https://t3.ftcdn.net/jpg/01/44/64/36/500_F_144643697_GJRUBtGc55KYSMpyg1Kucb9yJzvMQooW.jpg\n",
    "!wget -q -O northern-flicker-1.jpg https://upload.wikimedia.org/wikipedia/commons/5/5c/Northern_Flicker_%28Red-shafted%29.jpg\n",
    "!wget -q -O northern-cardinal-1.jpg https://cdn.pixabay.com/photo/2013/03/19/04/42/bird-94957_960_720.jpg\n",
    "!wget -q -O blue-jay-1.jpg https://cdn12.picryl.com/photo/2016/12/31/blue-jay-bird-feather-animals-b8ee04-1024.jpg\n",
    "!wget -q -O hummingbird-1.jpg http://res.freestockphotos.biz/pictures/17/17875-hummingbird-close-up-pv.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our endpoint expects the request payload to be an image sent as bytes within the request body. Here's a utility function to generate the requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_request_payload(img_loc) :\n",
    "\n",
    "    with open(img_loc, 'rb') as image:\n",
    "        f = image.read()\n",
    "        b = bytearray(f)\n",
    "        \n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model server implementation returns responses in JSON format. Here's a utility function that retrieves the bytes from the JSON payload, and restructures them back into tensor outputs corresponding to the class, score and bounding box predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "CID_END_OFFSET = 100\n",
    "SCORE_END_OFFSET = 200\n",
    "BBOX_END_OFFSET = 600\n",
    "Y = 100\n",
    "N_SCORES = 1\n",
    "N_CID = 1\n",
    "N_BBOX_POINTS = 4\n",
    "\n",
    "def parse_response_paylod(resp) :\n",
    "    \n",
    "    stacked_tensors = np.frombuffer(resp['Body'].read(), dtype=\"float32\")\n",
    "    #stacked_tensors = np.frombuffer(resp, dtype=\"float32\")\n",
    "    t= np.split(stacked_tensors,[CID_END_OFFSET,SCORE_END_OFFSET,BBOX_END_OFFSET])\n",
    "\n",
    "    c= mx.nd.array(t[0].reshape(Y,N_SCORES))\n",
    "    s= mx.nd.array(t[1].reshape(Y,N_CID))\n",
    "    bb= mx.nd.array(t[2].reshape(Y,N_BBOX_POINTS))\n",
    "    \n",
    "    return c,s,bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to detect the birds on the collection of images that we've downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (503) from model with message \"{\n  \"code\": 503,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Prediction failed\"\n}\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/dylantong-yolov3-rt-ep-2020-08-28-20-06-30 in account 407247006381 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-4263e3423578>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtest_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_IMAGES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-80-4263e3423578>\u001b[0m in \u001b[0;36mtest_endpoint\u001b[0;34m(img_file_list)\u001b[0m\n\u001b[1;32m     12\u001b[0m         response = runtime.invoke_endpoint(EndpointName='dylantong-yolov3-rt-ep-2020-08-28-20-06-30',\n\u001b[1;32m     13\u001b[0m                                             \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'image/jpeg'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                             Body=b)\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_response_paylod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (503) from model with message \"{\n  \"code\": 503,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Prediction failed\"\n}\n\". See https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#logEventViewer:group=/aws/sagemaker/Endpoints/dylantong-yolov3-rt-ep-2020-08-28-20-06-30 in account 407247006381 for more information."
     ]
    }
   ],
   "source": [
    "TEST_IMAGES = ['multi-goldfinch-1.jpg','northern-flicker-1.jpg', \n",
    "          'northern-cardinal-1.jpg','blue-jay-1.jpg','hummingbird-1.jpg']\n",
    "CLASSES = ['Cardinal','Northern_Flicker','American_Goldfinch', 'Ruby_throated_Hummingbird','Blue_Jay']\n",
    "runtime = boto3.client(service_name='runtime.sagemaker')\n",
    "\n",
    "def test_endpoint(img_file_list) :\n",
    "    \n",
    "    for img in img_file_list :\n",
    "\n",
    "        b = generate_request_payload(img)\n",
    "        \n",
    "        response = runtime.invoke_endpoint(EndpointName='dylantong-yolov3-rt-ep-2020-08-28-20-06-30',\n",
    "                                            ContentType='image/jpeg',\n",
    "                                            Body=b)\n",
    "\n",
    "        c,s,bb = parse_response_paylod(response)\n",
    "\n",
    "        trans_X, trans_image = gcv.data.transforms.presets.yolo.transform_test(mx.img.imdecode(b), 512)\n",
    "        ax = viz.plot_bbox(trans_image, bb, s,c, class_names=CLASSES)\n",
    "        plt.show()\n",
    "    \n",
    "test_endpoint(TEST_IMAGES)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "Here we delete the SageMaker endpoint, as we will no longer be performing any inferences.  This is an important step, as your account is billed for the amount of time an endpoint is running, even when it is idle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(yolo_rt_ep.endpoint)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (MXNet GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/mxnet-1.6-gpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
